diff --git a/.gitignore b/.gitignore
index 52ebf60..3138dfa 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,3 @@
-__pycache__
-exp*
+__pycache__
+exp*
 *video*
\ No newline at end of file
diff --git a/LICENSE b/LICENSE
index 261eeb9..29f81d8 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,201 +1,201 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/README.md b/README.md
index fd9a685..b2e1fae 100644
--- a/README.md
+++ b/README.md
@@ -1,11 +1,11 @@
-# DI-adventure
-
-Decision intelligence adventure for beginners, have fun and explore it!
-
-# Adventure List
-|  No  |                Environment               |                 Algorithm               |         Visualization            |                   Docs and Related Links                   |
-| :--: | :--------------------------------------: | :---------------------------------: | :--------------------------------:|:---------------------------------------------------------: |
-|  1   |       [mario](https://github.com/Kautenja/gym-super-mario-bros)    | [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)   | ![mario](./mario_dqn/assets/mario.gif)     |  [DQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)<br>[DQN中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/dqn_zh.html)|
-
-# License
-DI-adventure is released under the Apache 2.0 license.
+# DI-adventure
+
+Decision intelligence adventure for beginners, have fun and explore it!
+
+# Adventure List
+|  No  |                Environment               |                 Algorithm               |         Visualization            |                   Docs and Related Links                   |
+| :--: | :--------------------------------------: | :---------------------------------: | :--------------------------------:|:---------------------------------------------------------: |
+|  1   |       [mario](https://github.com/Kautenja/gym-super-mario-bros)    | [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)   | ![mario](./mario_dqn/assets/mario.gif)     |  [DQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)<br>[DQN中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/dqn_zh.html)|
+
+# License
+DI-adventure is released under the Apache 2.0 license.
diff --git a/mario_dqn/README.md b/mario_dqn/README.md
index 6b8ff65..cc82f49 100644
--- a/mario_dqn/README.md
+++ b/mario_dqn/README.md
@@ -1,195 +1,195 @@
-# 强化学习大作业代码配置与运行
-> 同学们不要对于RL背后的数学原理和复杂的代码逻辑感到困扰，首先是本次大作业会很少涉及到这一部分，仓库中对这一部分都有着良好的封装；其次是有问题（原理或者代码实现上的）可以随时提问一起交流，方式包括但不限于：
-> - github issue: https://github.com/opendilab/DI-adventure/issues
-> - 课程微信群
-> - 开发者邮箱: opendilab@pjlab.org.cn
-
-## 1. Baseline 代码获取与环境安装
-> mario环境的安装教程在网络学堂上，可以自行取用，需要注意：
-> 1. 请选择3.8版本的python以避免不必要的版本问题；
-> 2. 通过键盘与环境交互需要有可以用于渲染的显示设备，大部分服务器不能胜任，可以选择本地设备或者暂时跳过这一步，对后面没有影响；
-> 3. GPU服务器对于强化学习大作业是必须的，如果没有足够的计算资源（笔记本电脑可能难以承担）可能无法顺利完成所有实验；
-### 深度学习框架 PyTorch 安装
-这一步有网上有非常多的教程，请自行搜索学习，这里不予赘述。
-> 请安装 1.10.0 版本以避免不必要的环境问题
-### opencv-python 安装
-- 在对特征空间的修改中需要对马里奥游戏传回的图像进行处理，代码中使用的是 OpenCV 工具包，安装方法如下
-```bash
-pip install opencv-python
-```
-### Baseline 代码获取
-- 这次课程专门创建了 DI-advanture 仓库作为算法 baseline，推荐通过以下方式获取：
-```bash
-git clone https://github.com/opendilab/DI-adventure
-```
-如果出现网络问题，也可以直接去到 DI-advanture 的仓库手动下载后解压。这样做的缺陷是需要手动初始化 git 与设置远端仓库地址：
-```bash
-# 如果您是通过手动解压的方式才需要执行以下内容
-git init
-git add * && git commit -m 'init repo'
-git remote set-url origin https://github.com/opendilab/DI-adventure.git
-```
-推荐使用 git 作为代码管理工具，记录每一次的修改，推荐 [git 教程](https://www.liaoxuefeng.com/wiki/896043488029600)。
-### 强化学习库 DI-engine 安装
-- 由于这次大作业的目标不是强化学习算法，因此代码中使用了开源强化学习库 DI-engine 作为具体的强化学习算法实现，安装方法如下：
-```bash
-# clone主分支到本地
-git clone https://github.com/opendilab/DI-engine.git
-cd DI-engine
-git checkout 4c607d400d3290a27ad1e5b7fa8eeb4c2a1a4745
-pip install -e .
-```
-- (OPTIONAL)由于DI-adventure在不断更新，如果您目前使用的是老版本的DI-adventure，可能需要通过以下方式同步更新：
-```bash
-# 1. 更新DI-engine
-cd DI-engine
-git pull origin main
-git checkout 4c607d400d3290a27ad1e5b7fa8eeb4c2a1a4745
-pip install -e .
-# 2. 更新DI-adventure
-cd DI-adventure
-# 确认'origin'指向远端仓库‘git@github.com:opendilab/DI-adventure.git’
-git remote -v
-# 以下这步如果出现各种例如merge conflict问题，可以借助互联网或咨询助教帮助解决。
-# 或者直接重新安装DI-adventure，注意保存自己的更改。
-git pull origin main
-```
-- 修改 gym 版本
-```bash
-# DI-engine这里可能会将gym版本改为0.25.2，需要手动改回来
-pip install gym==0.25.1
-```
-- 安装grad-cam以保存CAM（Class Activation Mapping，类别激活映射图）
-```bash
-pip install grad-cam
-```
-## 2. Baseline 代码运行
-- 项目结构
-```bash
-.
-├── LICENSE
-├── mario_dqn                               --> 本次大作业相关代码：利用DQN算法训练《超级马里奥兄弟》智能体
-│   ├── assets
-│   │   ├── dqn.png                         --> 流程示意图
-│   │   └── mario.gif                       --> mario游戏gif示意图
-│   ├── evaluate.py                         --> 智能体评估函数
-│   ├── __init__.py
-│   ├── mario_dqn_main.py                   --> 智能体训练入口，包含训练的逻辑
-│   ├── mario_dqn_config.py                 --> 智能体配置文件，包含参数信息       
-│   ├── model.py                            --> 神经网络结构定义文件
-│   ├── policy.py                           --> 策略逻辑文件，包含经验收集、智能体评估、模型训练的逻辑
-│   ├── README.md
-│   ├── requirements.txt                    --> 项目依赖目录
-│   └── wrapper.py                          --> 各式各样的装饰器实现
-└── README.md
-```
-
-- 神经网络结构
-![](assets/dqn.png)
-- 代码运行
-
-推荐使用[tmux](http://www.ruanyifeng.com/blog/2019/10/tmux.html)来管理实验。
-```bash
-cd DI-adventure/mario_dqn
-# 对于每组参数，如果有服务器，计算资源充足，推荐设置三个种子（例如seed=0/1/2）进行3组实验，否则先运行一个seed。
-python3 -u mario_dqn_main.py -s <SEED> -v <VERSION> -a <ACTION SET> -o <FRAME NUMBER>
-# 以下命令的含义是，设置seed=0，游戏版本v0，动作数目为7（即SIMPLE_MOVEMENT），观测通道数目为1（即不进行叠帧）进行训练。
-python3 -u mario_dqn_main.py -s 0 -v 0 -a 7 -o 1
-```
-训练到与环境交互3,000,000 steps时程序会自动停止，运行时长依据机器性能在3小时到10小时不等，这里如果计算资源充足的同学可以改成5,000,000 steps（main函数中设置max_env_step参数）。程序运行期间可以看看代码逻辑。
-## 3. 智能体性能评估
-## tensorboard 查看训练过程中的曲线
-- 首先安装 tensorboard 工具：
-```bash
-pip install tensorboard
-```
-- 查看训练日志：
-```bash
-tensorboard --logdir <exp_dir>
-```
-### tensorboard 中指标含义如下
-tensorboard结果分为 buffer, collector, evaluator, learner 四个部分，以\_iter结尾表明横轴是训练迭代iteration数目，以\_step结尾表明横轴是与环境交互步数step。
-一般而言会更加关注与环境交互的步数，即 collector/evaluator/learner\_step。
-#### evaluator
-评估过程的一些结果，最为重要！展开evaluator_step，主要关注：
-- reward_mean：即为任务书中的“episode return”。代表评估分数随着与环境交互交互步数的变化，一般而言，整体上随着交互步数越多（训练了越久），分数越高。
-- avg_envstep_per_episode：每局游戏（一个episode）马里奥平均行动了多少step，一般而言认为比较长一点会好；如果很快死亡的话envstep就会很短，但是也不排除卡在某个地方导致超时的情况；如果在某一step突然上升，说明学到了某一个很有用的动作使得过了某一个难关，例如看到坑学会了跳跃。
-#### collector
-探索过程的一些结果，展开collector_step，其内容和evaluator_step基本一致，但是由于探索过程加了噪声（epsilon-greedy），一般reward_mean会低一些。
-#### learner
-学习过程的一些结果，展开learner_step：
-- q_value_avg：Q-Network的输出变化，在稳定后一般是稳固上升；
-- target_q_value_avg：Target Q-Network的输出变化，和Q-Network基本上一致；
-- total_loss_avg：损失曲线，一般不爆炸就不用管，这一点和监督学习有很大差异，思考一下是什么造成了这种差异？
-- cur_lr_avg：学习率变化，由于默认不使用学习率衰减，因此会是一条直线；
-#### buffer
-DQN是off-policy算法，因此会有一个replay buffer用以保存数据，本次大作业不用太关注buffer；
-
-总体而言，看看evaluator_step/reward_mean，目标是在尽可能少的环境交互步数能达到尽可能高的回报，一般而言3000分可以认为通关1-1。
-
-## 对智能体性能进行评估，并保存录像：
-```bash
-python3 -u evaluate.py -ckpt <CHECKPOINT_PATH> -v <VERSION> -a <ACTION SET> -o <FRAME NUMBER>
-```
-- 此外该命令还会保存评估时的游戏录像（eval_videos/rl-video-xxx.mp4），与类别激活映射CAM（eval_videos/merged.mp4），以供查看，请确保您的 ffmpeg 软件可用。
-- 评估时由于mario环境是确定性的（这个比较特殊），同时DQN是确定性（deterministic）策略，因此结果不会因为seed的改变而改变。但训练时由于需要探索，因此多个seed是必要的。
-
-具体而言，对于你想要分析的智能体，从：
-1. tensorboard结果曲线；
-2. 游戏录像；
-3. 类别激活映射CAM；
-
-三个角度入手分析即可。
-# 4. 特征处理
-- 包括对于观测空间（observation space）、动作空间（action space）和奖励空间（reward space）的处理；
-- 这一部分主要使用 wrapper 来实现，什么是 wrapper 可以参考：
-    1. [如何自定义一个 ENV WRAPPER](https://di-engine-docs.readthedocs.io/zh_CN/latest/04_best_practice/env_wrapper_zh.html)
-    2. [Gym Documentation Wrappers](https://www.gymlibrary.dev/api/wrappers/)
-
-可以对以下特征空间更改进行尝试：
-### 观测空间（observation space）
-- 图像降采样，即将游戏版本从`v0`更改为`v1`，游戏版本的内容请参照[mario游戏仓库](https://github.com/Kautenja/gym-super-mario-bros)：`-v 1`；
-- 堆叠四帧作为输入，即输入变为`(4,84,84)`的图像：`-o 4`；
-    - 叠帧wrapper可以将连续多帧的图像叠在一起送入网络，补充mario运动的速度等单帧图像无法获取的信息；
-- 图像内容简化（尝试游戏版本`v2`、`v3`的效果）：`-v 2/3`；
-### 动作空间（action space）
-- 动作简化，将 `SIMPLE_ACTION` 替换为 `[['right'], ['right', 'A']]`：`-a 2`；
-    - mario提供了不同的[按键组合](https://github.com/Kautenja/gym-super-mario-bros/blob/master/gym_super_mario_bros/actions.py)，有时候简化动作种类能有效降低训练前期学习的困难，但可能降低操作上限；
-- 增加动作的多样性，将 `SIMPLE_ACTION` 替换为 `COMPLEX_MOVEMENT`：`-a 12`；
-    - 也许能提高上限；
-- 粘性动作 sticky action（给环境添加 `StickyActionWrapper`，方式和其它自带的 wrapper 相同，即`lambda env: StickyActionWrapper(env)`）
-    - 粘性动作的含义是，智能体有一定概率直接采用上一帧的动作，可以增加环境的随机性；
-### （拓展）奖励空间（reward space）
-
-目前mario的奖励请参照[mario游戏仓库](https://github.com/Kautenja/gym-super-mario-bros)
-- 尝试给予金币奖励（给环境添加 `CoinRewardWrapper`，方式和其它自带的 wrapper 相同）；
-    - 能否让mario学会吃金币呢；
-- 稀疏 reward，只有死亡和过关才给reward（给环境添加 `SparseRewardWrapper`，方式和其它自带的 wrapper 相同）
-    - 完全目标导向。稀疏奖励是强化学习想要落地必须克服的问题，有时候在结果出来前无法判断中途的某个动作的好坏；
-
-**由于同学们计算资源可能不是特别充分，这里提示一下，图像降采样、图像内容简化、叠帧、动作简化是比较有效能提升性能的方法！**
-
-以下是非常缺少计算资源和时间，最小限度需要完成的实验任务：
-1. baseline（即`v0+SIMPLE MOVEMENT+1 Frame`）跑一个seed看看结果；
-2. 尝试简化动作空间的同时进行叠帧（即`v0+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看；
-3. 观测空间去除冗余信息（即`v1+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看，如果没通关则试试换个seed；
-4. 从tensorboard、可视化、CAM以及对特征空间的修改角度分析通关/没有通过的原因。
-
-对于有充足计算资源的同学，推荐增加实验的seed、延长实验步长到5M、更换其它游戏版本、尝试其它动作观测空间组合，使用其它的wrapper、以及free style；
-
----
-**新增：一些实验[结果](https://github.com/opendilab/DI-adventure/blob/results/mario_dqn/README.md)供大家参考！**
-**新增：分析[思路/范例](https://github.com/opendilab/DI-adventure/tree/analysis/mario_dqn)供大家参考！**
-# 对于大作业任务书的一些补充说明：
-**如果不知道接下来要做什么了，请参考任务书或咨询助教！！！**
-- “3.2【baseline 跑通】（3）训练出能够通关简单级别关卡（1-1 ~~，1-2~~ ）的智能体”。 考虑到算力等因素，大家只需要关注关卡1-1即可。
-- “3.2【baseline 跑通】~~（5）查看网络预测的 Q 值与实际 Q 值，判断当前是否存在高估或者低估问题;~~”。没有提供实际Q值，这一点要求去掉。
-- “3.4【结果分析】20 分”，不需要每一组参数都分析，选择有代表性或你想要分析的参数与wrapper组合，从tensorboard结果曲线、评估视频与CAM激活图三个方面出发分析即可。由于视频无法放入实验报告与海报，对有意思的部分进行截图插入即可。
-
-# Update
-## 11.30 
-- 修复了evaluate.py以及mario_dqn_main.py中，预设动作维度不正确的bug，该bug曾经导致无法使用COMPLEX_MOVEMENT。感谢邹岷强同学的反馈。
-## 12.08
-- 修复了因为DI-engine更新导致的FinalEvalRewardEnv wrapper不可用的bug，感谢吴天鹤同学的反馈。
-## 12.09
-- 润色了一下注释，不影响程序运行。
+# 强化学习大作业代码配置与运行
+> 同学们不要对于RL背后的数学原理和复杂的代码逻辑感到困扰，首先是本次大作业会很少涉及到这一部分，仓库中对这一部分都有着良好的封装；其次是有问题（原理或者代码实现上的）可以随时提问一起交流，方式包括但不限于：
+> - github issue: https://github.com/opendilab/DI-adventure/issues
+> - 课程微信群
+> - 开发者邮箱: opendilab@pjlab.org.cn
+
+## 1. Baseline 代码获取与环境安装
+> mario环境的安装教程在网络学堂上，可以自行取用，需要注意：
+> 1. 请选择3.8版本的python以避免不必要的版本问题；
+> 2. 通过键盘与环境交互需要有可以用于渲染的显示设备，大部分服务器不能胜任，可以选择本地设备或者暂时跳过这一步，对后面没有影响；
+> 3. GPU服务器对于强化学习大作业是必须的，如果没有足够的计算资源（笔记本电脑可能难以承担）可能无法顺利完成所有实验；
+### 深度学习框架 PyTorch 安装
+这一步有网上有非常多的教程，请自行搜索学习，这里不予赘述。
+> 请安装 1.10.0 版本以避免不必要的环境问题
+### opencv-python 安装
+- 在对特征空间的修改中需要对马里奥游戏传回的图像进行处理，代码中使用的是 OpenCV 工具包，安装方法如下
+```bash
+pip install opencv-python
+```
+### Baseline 代码获取
+- 这次课程专门创建了 DI-advanture 仓库作为算法 baseline，推荐通过以下方式获取：
+```bash
+git clone https://github.com/opendilab/DI-adventure
+```
+如果出现网络问题，也可以直接去到 DI-advanture 的仓库手动下载后解压。这样做的缺陷是需要手动初始化 git 与设置远端仓库地址：
+```bash
+# 如果您是通过手动解压的方式才需要执行以下内容
+git init
+git add * && git commit -m 'init repo'
+git remote set-url origin https://github.com/opendilab/DI-adventure.git
+```
+推荐使用 git 作为代码管理工具，记录每一次的修改，推荐 [git 教程](https://www.liaoxuefeng.com/wiki/896043488029600)。
+### 强化学习库 DI-engine 安装
+- 由于这次大作业的目标不是强化学习算法，因此代码中使用了开源强化学习库 DI-engine 作为具体的强化学习算法实现，安装方法如下：
+```bash
+# clone主分支到本地
+git clone https://github.com/opendilab/DI-engine.git
+cd DI-engine
+git checkout 4c607d400d3290a27ad1e5b7fa8eeb4c2a1a4745
+pip install -e .
+```
+- (OPTIONAL)由于DI-adventure在不断更新，如果您目前使用的是老版本的DI-adventure，可能需要通过以下方式同步更新：
+```bash
+# 1. 更新DI-engine
+cd DI-engine
+git pull origin main
+git checkout 4c607d400d3290a27ad1e5b7fa8eeb4c2a1a4745
+pip install -e .
+# 2. 更新DI-adventure
+cd DI-adventure
+# 确认'origin'指向远端仓库‘git@github.com:opendilab/DI-adventure.git’
+git remote -v
+# 以下这步如果出现各种例如merge conflict问题，可以借助互联网或咨询助教帮助解决。
+# 或者直接重新安装DI-adventure，注意保存自己的更改。
+git pull origin main
+```
+- 修改 gym 版本
+```bash
+# DI-engine这里可能会将gym版本改为0.25.2，需要手动改回来
+pip install gym==0.25.1
+```
+- 安装grad-cam以保存CAM（Class Activation Mapping，类别激活映射图）
+```bash
+pip install grad-cam
+```
+## 2. Baseline 代码运行
+- 项目结构
+```bash
+.
+├── LICENSE
+├── mario_dqn                               --> 本次大作业相关代码：利用DQN算法训练《超级马里奥兄弟》智能体
+│   ├── assets
+│   │   ├── dqn.png                         --> 流程示意图
+│   │   └── mario.gif                       --> mario游戏gif示意图
+│   ├── evaluate.py                         --> 智能体评估函数
+│   ├── __init__.py
+│   ├── mario_dqn_main.py                   --> 智能体训练入口，包含训练的逻辑
+│   ├── mario_dqn_config.py                 --> 智能体配置文件，包含参数信息       
+│   ├── model.py                            --> 神经网络结构定义文件
+│   ├── policy.py                           --> 策略逻辑文件，包含经验收集、智能体评估、模型训练的逻辑
+│   ├── README.md
+│   ├── requirements.txt                    --> 项目依赖目录
+│   └── wrapper.py                          --> 各式各样的装饰器实现
+└── README.md
+```
+
+- 神经网络结构
+![](assets/dqn.png)
+- 代码运行
+
+推荐使用[tmux](http://www.ruanyifeng.com/blog/2019/10/tmux.html)来管理实验。
+```bash
+cd DI-adventure/mario_dqn
+# 对于每组参数，如果有服务器，计算资源充足，推荐设置三个种子（例如seed=0/1/2）进行3组实验，否则先运行一个seed。
+python3 -u mario_dqn_main.py -s <SEED> -v <VERSION> -a <ACTION SET> -o <FRAME NUMBER>
+# 以下命令的含义是，设置seed=0，游戏版本v0，动作数目为7（即SIMPLE_MOVEMENT），观测通道数目为1（即不进行叠帧）进行训练。
+python3 -u mario_dqn_main.py -s 0 -v 0 -a 7 -o 1
+```
+训练到与环境交互3,000,000 steps时程序会自动停止，运行时长依据机器性能在3小时到10小时不等，这里如果计算资源充足的同学可以改成5,000,000 steps（main函数中设置max_env_step参数）。程序运行期间可以看看代码逻辑。
+## 3. 智能体性能评估
+## tensorboard 查看训练过程中的曲线
+- 首先安装 tensorboard 工具：
+```bash
+pip install tensorboard
+```
+- 查看训练日志：
+```bash
+tensorboard --logdir <exp_dir>
+```
+### tensorboard 中指标含义如下
+tensorboard结果分为 buffer, collector, evaluator, learner 四个部分，以\_iter结尾表明横轴是训练迭代iteration数目，以\_step结尾表明横轴是与环境交互步数step。
+一般而言会更加关注与环境交互的步数，即 collector/evaluator/learner\_step。
+#### evaluator
+评估过程的一些结果，最为重要！展开evaluator_step，主要关注：
+- reward_mean：即为任务书中的“episode return”。代表评估分数随着与环境交互交互步数的变化，一般而言，整体上随着交互步数越多（训练了越久），分数越高。
+- avg_envstep_per_episode：每局游戏（一个episode）马里奥平均行动了多少step，一般而言认为比较长一点会好；如果很快死亡的话envstep就会很短，但是也不排除卡在某个地方导致超时的情况；如果在某一step突然上升，说明学到了某一个很有用的动作使得过了某一个难关，例如看到坑学会了跳跃。
+#### collector
+探索过程的一些结果，展开collector_step，其内容和evaluator_step基本一致，但是由于探索过程加了噪声（epsilon-greedy），一般reward_mean会低一些。
+#### learner
+学习过程的一些结果，展开learner_step：
+- q_value_avg：Q-Network的输出变化，在稳定后一般是稳固上升；
+- target_q_value_avg：Target Q-Network的输出变化，和Q-Network基本上一致；
+- total_loss_avg：损失曲线，一般不爆炸就不用管，这一点和监督学习有很大差异，思考一下是什么造成了这种差异？
+- cur_lr_avg：学习率变化，由于默认不使用学习率衰减，因此会是一条直线；
+#### buffer
+DQN是off-policy算法，因此会有一个replay buffer用以保存数据，本次大作业不用太关注buffer；
+
+总体而言，看看evaluator_step/reward_mean，目标是在尽可能少的环境交互步数能达到尽可能高的回报，一般而言3000分可以认为通关1-1。
+
+## 对智能体性能进行评估，并保存录像：
+```bash
+python3 -u evaluate.py -ckpt <CHECKPOINT_PATH> -v <VERSION> -a <ACTION SET> -o <FRAME NUMBER>
+```
+- 此外该命令还会保存评估时的游戏录像（eval_videos/rl-video-xxx.mp4），与类别激活映射CAM（eval_videos/merged.mp4），以供查看，请确保您的 ffmpeg 软件可用。
+- 评估时由于mario环境是确定性的（这个比较特殊），同时DQN是确定性（deterministic）策略，因此结果不会因为seed的改变而改变。但训练时由于需要探索，因此多个seed是必要的。
+
+具体而言，对于你想要分析的智能体，从：
+1. tensorboard结果曲线；
+2. 游戏录像；
+3. 类别激活映射CAM；
+
+三个角度入手分析即可。
+# 4. 特征处理
+- 包括对于观测空间（observation space）、动作空间（action space）和奖励空间（reward space）的处理；
+- 这一部分主要使用 wrapper 来实现，什么是 wrapper 可以参考：
+    1. [如何自定义一个 ENV WRAPPER](https://di-engine-docs.readthedocs.io/zh_CN/latest/04_best_practice/env_wrapper_zh.html)
+    2. [Gym Documentation Wrappers](https://www.gymlibrary.dev/api/wrappers/)
+
+可以对以下特征空间更改进行尝试：
+### 观测空间（observation space）
+- 图像降采样，即将游戏版本从`v0`更改为`v1`，游戏版本的内容请参照[mario游戏仓库](https://github.com/Kautenja/gym-super-mario-bros)：`-v 1`；
+- 堆叠四帧作为输入，即输入变为`(4,84,84)`的图像：`-o 4`；
+    - 叠帧wrapper可以将连续多帧的图像叠在一起送入网络，补充mario运动的速度等单帧图像无法获取的信息；
+- 图像内容简化（尝试游戏版本`v2`、`v3`的效果）：`-v 2/3`；
+### 动作空间（action space）
+- 动作简化，将 `SIMPLE_ACTION` 替换为 `[['right'], ['right', 'A']]`：`-a 2`；
+    - mario提供了不同的[按键组合](https://github.com/Kautenja/gym-super-mario-bros/blob/master/gym_super_mario_bros/actions.py)，有时候简化动作种类能有效降低训练前期学习的困难，但可能降低操作上限；
+- 增加动作的多样性，将 `SIMPLE_ACTION` 替换为 `COMPLEX_MOVEMENT`：`-a 12`；
+    - 也许能提高上限；
+- 粘性动作 sticky action（给环境添加 `StickyActionWrapper`，方式和其它自带的 wrapper 相同，即`lambda env: StickyActionWrapper(env)`）
+    - 粘性动作的含义是，智能体有一定概率直接采用上一帧的动作，可以增加环境的随机性；
+### （拓展）奖励空间（reward space）
+
+目前mario的奖励请参照[mario游戏仓库](https://github.com/Kautenja/gym-super-mario-bros)
+- 尝试给予金币奖励（给环境添加 `CoinRewardWrapper`，方式和其它自带的 wrapper 相同）；
+    - 能否让mario学会吃金币呢；
+- 稀疏 reward，只有死亡和过关才给reward（给环境添加 `SparseRewardWrapper`，方式和其它自带的 wrapper 相同）
+    - 完全目标导向。稀疏奖励是强化学习想要落地必须克服的问题，有时候在结果出来前无法判断中途的某个动作的好坏；
+
+**由于同学们计算资源可能不是特别充分，这里提示一下，图像降采样、图像内容简化、叠帧、动作简化是比较有效能提升性能的方法！**
+
+以下是非常缺少计算资源和时间，最小限度需要完成的实验任务：
+1. baseline（即`v0+SIMPLE MOVEMENT+1 Frame`）跑一个seed看看结果；
+2. 尝试简化动作空间的同时进行叠帧（即`v0+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看；
+3. 观测空间去除冗余信息（即`v1+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看，如果没通关则试试换个seed；
+4. 从tensorboard、可视化、CAM以及对特征空间的修改角度分析通关/没有通过的原因。
+
+对于有充足计算资源的同学，推荐增加实验的seed、延长实验步长到5M、更换其它游戏版本、尝试其它动作观测空间组合，使用其它的wrapper、以及free style；
+
+---
+**新增：一些实验[结果](https://github.com/opendilab/DI-adventure/blob/results/mario_dqn/README.md)供大家参考！**
+**新增：分析[思路/范例](https://github.com/opendilab/DI-adventure/tree/analysis/mario_dqn)供大家参考！**
+# 对于大作业任务书的一些补充说明：
+**如果不知道接下来要做什么了，请参考任务书或咨询助教！！！**
+- “3.2【baseline 跑通】（3）训练出能够通关简单级别关卡（1-1 ~~，1-2~~ ）的智能体”。 考虑到算力等因素，大家只需要关注关卡1-1即可。
+- “3.2【baseline 跑通】~~（5）查看网络预测的 Q 值与实际 Q 值，判断当前是否存在高估或者低估问题;~~”。没有提供实际Q值，这一点要求去掉。
+- “3.4【结果分析】20 分”，不需要每一组参数都分析，选择有代表性或你想要分析的参数与wrapper组合，从tensorboard结果曲线、评估视频与CAM激活图三个方面出发分析即可。由于视频无法放入实验报告与海报，对有意思的部分进行截图插入即可。
+
+# Update
+## 11.30 
+- 修复了evaluate.py以及mario_dqn_main.py中，预设动作维度不正确的bug，该bug曾经导致无法使用COMPLEX_MOVEMENT。感谢邹岷强同学的反馈。
+## 12.08
+- 修复了因为DI-engine更新导致的FinalEvalRewardEnv wrapper不可用的bug，感谢吴天鹤同学的反馈。
+## 12.09
+- 润色了一下注释，不影响程序运行。
diff --git a/mario_dqn/evaluate.py b/mario_dqn/evaluate.py
index 40c6648..8e6cbc0 100644
--- a/mario_dqn/evaluate.py
+++ b/mario_dqn/evaluate.py
@@ -1,95 +1,95 @@
-"""
-智能体评估函数
-"""
-import torch
-from ding.utils import set_pkg_seed
-from mario_dqn_config import mario_dqn_config, mario_dqn_create_config
-from model import DQN
-from policy import DQNPolicy
-from ding.config import compile_config
-from ding.envs import DingEnvWrapper
-import gym_super_mario_bros
-from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
-from nes_py.wrappers import JoypadSpace
-from wrapper import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
-    FinalEvalRewardEnv, RecordCAM
-
-action_dict = {2: [["right"], ["right", "A"]], 7: SIMPLE_MOVEMENT, 12: COMPLEX_MOVEMENT}
-action_nums = [2, 7, 12]
-
-
-def wrapped_mario_env(model, cam_video_path, version=0, action=2, obs=1):
-    return DingEnvWrapper(
-        JoypadSpace(gym_super_mario_bros.make("SuperMarioBros-1-1-v"+str(version)), action_dict[int(action)]),
-        cfg={
-            'env_wrapper': [
-                lambda env: MaxAndSkipWrapper(env, skip=4),
-                lambda env: WarpFrameWrapper(env, size=84),
-                lambda env: ScaledFloatFrameWrapper(env),
-                lambda env: FrameStackWrapper(env, n_frames=obs),
-                lambda env: FinalEvalRewardEnv(env),
-                lambda env: RecordCAM(env, cam_model=model, video_folder=cam_video_path)
-            ]
-        }
-    )
-
-
-def evaluate(args, state_dict, seed, video_dir_path, eval_times):
-    # 加载配置
-    cfg = compile_config(mario_dqn_config, create_cfg=mario_dqn_create_config, auto=True, save_cfg=False)
-    # 实例化DQN模型
-    model = DQN(**cfg.policy.model)
-    # 加载模型权重文件
-    model.load_state_dict(state_dict['model'])
-    # 生成环境
-    env = wrapped_mario_env(model, args.replay_path, args.version, args.action, args.obs)
-    # 实例化DQN策略
-    policy = DQNPolicy(cfg.policy, model=model).eval_mode
-    # 设置seed
-    env.seed(seed)
-    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)
-    # 保存录像
-    env.enable_save_replay(video_dir_path)
-    eval_reward_list = []
-    # 评估
-    for n in range(eval_times):
-        # 环境重置，返回初始观测
-        obs = env.reset()
-        eval_reward = 0
-        while True:
-            # 策略根据观测返回所有动作的Q值以及Q值最大的动作
-            Q = policy.forward({0: obs})
-            # 获取动作
-            action = Q[0]['action'].item()
-            # 将动作传入环境，环境返回下一帧信息
-            obs, reward, done, info = env.step(action)
-            eval_reward += reward
-            if done or info['time'] < 250:
-                print(info)
-                eval_reward_list.append(eval_reward)
-                break
-        print('During {}th evaluation, the total reward your mario got is {}'.format(n, eval_reward))
-    print('Eval is over! The performance of your RL policy is {}'.format(sum(eval_reward_list) / len(eval_reward_list)))
-    print("Your mario video is saved in {}".format(video_dir_path))
-    try:
-        del env
-    except Exception:
-        pass
-
-
-if __name__ == "__main__":
-    import argparse
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--seed", "-s", type=int, default=0)
-    parser.add_argument("--checkpoint", "-ckpt", type=str, default='./exp/v0_1a_7f_seed0/ckpt/ckpt_best.pth.tar')
-    parser.add_argument("--replay_path", "-rp", type=str, default='./eval_videos')
-    parser.add_argument("--version", "-v", type=int, default=0, choices=[0,1,2,3])
-    parser.add_argument("--action", "-a", type=int, default=7, choices=[2,7,12])
-    parser.add_argument("--obs", "-o", type=int, default=1, choices=[1,4])
-    args = parser.parse_args()
-    mario_dqn_config.policy.model.obs_shape=[args.obs, 84, 84]
-    mario_dqn_config.policy.model.action_shape=args.action
-    ckpt_path = args.checkpoint
-    video_dir_path = args.replay_path
-    state_dict = torch.load(ckpt_path, map_location='cpu')
-    evaluate(args, state_dict=state_dict, seed=args.seed, video_dir_path=video_dir_path, eval_times=1)
+"""
+智能体评估函数
+"""
+import torch
+from ding.utils import set_pkg_seed
+from mario_dqn_config import mario_dqn_config, mario_dqn_create_config
+from model import DQN
+from policy import DQNPolicy
+from ding.config import compile_config
+from ding.envs import DingEnvWrapper
+import gym_super_mario_bros
+from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
+from nes_py.wrappers import JoypadSpace
+from wrapper import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
+    FinalEvalRewardEnv, RecordCAM
+
+action_dict = {2: [["right"], ["right", "A"]], 4: [["right"], ["right", "A"], ["left"], ["left", "A"]], 7: SIMPLE_MOVEMENT, 12: COMPLEX_MOVEMENT}
+action_nums = [2, 7, 12]
+
+
+def wrapped_mario_env(model, cam_video_path, version=0, action=2, obs=1):
+    return DingEnvWrapper(
+        JoypadSpace(gym_super_mario_bros.make("SuperMarioBros-1-1-v"+str(version)), action_dict[int(action)]),
+        cfg={
+            'env_wrapper': [
+                lambda env: MaxAndSkipWrapper(env, skip=4),
+                lambda env: WarpFrameWrapper(env, size=84),
+                lambda env: ScaledFloatFrameWrapper(env),
+                lambda env: FrameStackWrapper(env, n_frames=obs),
+                lambda env: FinalEvalRewardEnv(env),
+                lambda env: RecordCAM(env, cam_model=model, video_folder=cam_video_path)
+            ]
+        }
+    )
+
+
+def evaluate(args, state_dict, seed, video_dir_path, eval_times):
+    # 加载配置
+    cfg = compile_config(mario_dqn_config, create_cfg=mario_dqn_create_config, auto=True, save_cfg=False)
+    # 实例化DQN模型
+    model = DQN(**cfg.policy.model)
+    # 加载模型权重文件
+    model.load_state_dict(state_dict['model'])
+    # 生成环境
+    env = wrapped_mario_env(model, args.replay_path, args.version, args.action, args.obs)
+    # 实例化DQN策略
+    policy = DQNPolicy(cfg.policy, model=model).eval_mode
+    # 设置seed
+    env.seed(seed)
+    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)
+    # 保存录像
+    env.enable_save_replay(video_dir_path)
+    eval_reward_list = []
+    # 评估
+    for n in range(eval_times):
+        # 环境重置，返回初始观测
+        obs = env.reset()
+        eval_reward = 0
+        while True:
+            # 策略根据观测返回所有动作的Q值以及Q值最大的动作
+            Q = policy.forward({0: obs})
+            # 获取动作
+            action = Q[0]['action'].item()
+            # 将动作传入环境，环境返回下一帧信息
+            obs, reward, done, info = env.step(action)
+            eval_reward += reward
+            if done or info['time'] < 250:
+                print(info)
+                eval_reward_list.append(eval_reward)
+                break
+        print('During {}th evaluation, the total reward your mario got is {}'.format(n, eval_reward))
+    print('Eval is over! The performance of your RL policy is {}'.format(sum(eval_reward_list) / len(eval_reward_list)))
+    print("Your mario video is saved in {}".format(video_dir_path))
+    try:
+        del env
+    except Exception:
+        pass
+
+
+if __name__ == "__main__":
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--seed", "-s", type=int, default=0)
+    parser.add_argument("--checkpoint", "-ckpt", type=str, default='./exp/v0_1a_7f_seed0/ckpt/ckpt_best.pth.tar')
+    parser.add_argument("--replay_path", "-rp", type=str, default='./eval_videos')
+    parser.add_argument("--version", "-v", type=int, default=0, choices=[0,1,2,3])
+    parser.add_argument("--action", "-a", type=int, default=7, choices=[2,4 ,7,12])
+    parser.add_argument("--obs", "-o", type=int, default=1, choices=[1,4])
+    args = parser.parse_args()
+    mario_dqn_config.policy.model.obs_shape=[args.obs, 84, 84]
+    mario_dqn_config.policy.model.action_shape=args.action
+    ckpt_path = args.checkpoint
+    video_dir_path = args.replay_path
+    state_dict = torch.load(ckpt_path, map_location='cpu')
+    evaluate(args, state_dict=state_dict, seed=args.seed, video_dir_path=video_dir_path, eval_times=1)
diff --git a/mario_dqn/mario_dqn_config.py b/mario_dqn/mario_dqn_config.py
index d6b051b..5617389 100644
--- a/mario_dqn/mario_dqn_config.py
+++ b/mario_dqn/mario_dqn_config.py
@@ -1,76 +1,76 @@
-"""
-config 配置文件，这一部分主要包含一些超参数的配置，大家只用关注 model 中的参数即可
-"""
-from easydict import EasyDict
-
-mario_dqn_config = dict(
-    # 实验结果的存放路径
-    exp_name='exp/mario_dqn_seed0',
-    # mario环境相关
-    env=dict(
-        # 用来收集经验（experience）的mario环境的数目
-        # 请根据机器的性能自行增减
-        collector_env_num=8,
-        # 用来评估智能体性能的mario环境的数目
-        # 请根据机器的性能自行增减
-        evaluator_env_num=8,
-        # 评估轮次
-        n_evaluator_episode=8,
-        # 训练停止的分数（3000分可以认为通关1-1，停止训练以节省计算资源）
-        stop_value=3000
-    ),
-    policy=dict(
-        # 是否使用 CUDA 加速（必要）
-        cuda=True,
-        # 神经网络模型相关参数
-        model=dict(
-            # 网络输入的张量形状
-            obs_shape=[1, 84, 84],
-            # 有多少个可选动作
-            action_shape=7,
-            # 网络结构超参数
-            encoder_hidden_size_list=[32, 64, 128],
-            # 是否使用对决网络 Dueling Network
-            dueling=False,
-        ),
-        # n-step TD
-        nstep=3,
-        # 折扣系数 gamma
-        discount_factor=0.99,
-        # 训练相关参数
-        learn=dict(
-            # 每次利用相同的经验更新网络的次数
-            update_per_collect=10,
-            # batch size大小
-            batch_size=32,
-            # 学习率
-            learning_rate=0.0001,
-            # target Q-network更新频率
-            target_update_freq=500,
-        ),
-        # 收集经验相关，每次收集96个transition进行一次训练
-        collect=dict(n_sample=96, ),
-        # 评估相关，每2000个iteration评估一次
-        eval=dict(evaluator=dict(eval_freq=2000, )),
-        other=dict(
-            # epsilon-greedy算法
-            eps=dict(
-                type='exp',
-                start=1.,
-                end=0.05,
-                decay=250000,
-            ),
-            # replay buffer大小
-            replay_buffer=dict(replay_buffer_size=100000, ),
-        ),
-    ),
-)
-mario_dqn_config = EasyDict(mario_dqn_config)
-main_config = mario_dqn_config
-mario_dqn_create_config = dict(
-    env_manager=dict(type='subprocess'),
-    policy=dict(type='dqn'),
-)
-mario_dqn_create_config = EasyDict(mario_dqn_create_config)
-create_config = mario_dqn_create_config
+"""
+config 配置文件，这一部分主要包含一些超参数的配置，大家只用关注 model 中的参数即可
+"""
+from easydict import EasyDict
+
+mario_dqn_config = dict(
+    # 实验结果的存放路径
+    exp_name='exp/mario_dqn_seed0',
+    # mario环境相关
+    env=dict(
+        # 用来收集经验（experience）的mario环境的数目
+        # 请根据机器的性能自行增减
+        collector_env_num=8,
+        # 用来评估智能体性能的mario环境的数目
+        # 请根据机器的性能自行增减
+        evaluator_env_num=8,
+        # 评估轮次
+        n_evaluator_episode=8,
+        # 训练停止的分数（3000分可以认为通关1-1，停止训练以节省计算资源）
+        stop_value=3000
+    ),
+    policy=dict(
+        # 是否使用 CUDA 加速（必要）
+        cuda=True,
+        # 神经网络模型相关参数
+        model=dict(
+            # 网络输入的张量形状
+            obs_shape=[1, 84, 84],
+            # 有多少个可选动作
+            action_shape=7,
+            # 网络结构超参数
+            encoder_hidden_size_list=[32, 64, 128],
+            # 是否使用对决网络 Dueling Network
+            dueling=False,
+        ),
+        # n-step TD
+        nstep=3,
+        # 折扣系数 gamma
+        discount_factor=0.99,
+        # 训练相关参数
+        learn=dict(
+            # 每次利用相同的经验更新网络的次数
+            update_per_collect=10,
+            # batch size大小
+            batch_size=32,
+            # 学习率
+            learning_rate=0.0001,
+            # target Q-network更新频率
+            target_update_freq=500,
+        ),
+        # 收集经验相关，每次收集96个transition进行一次训练
+        collect=dict(n_sample=96, ),
+        # 评估相关，每2000个iteration评估一次
+        eval=dict(evaluator=dict(eval_freq=2000, )),
+        other=dict(
+            # epsilon-greedy算法
+            eps=dict(
+                type='exp',
+                start=1.,
+                end=0.05,
+                decay=250000,
+            ),
+            # replay buffer大小
+            replay_buffer=dict(replay_buffer_size=100000, ),
+        ),
+    ),
+)
+mario_dqn_config = EasyDict(mario_dqn_config)
+main_config = mario_dqn_config
+mario_dqn_create_config = dict(
+    env_manager=dict(type='subprocess'),
+    policy=dict(type='dqn'),
+)
+mario_dqn_create_config = EasyDict(mario_dqn_create_config)
+create_config = mario_dqn_create_config
 # you can run `python3 -u mario_dqn_main.py`
\ No newline at end of file
diff --git a/mario_dqn/mario_dqn_main.py b/mario_dqn/mario_dqn_main.py
index 3f019e1..dbae9c6 100644
--- a/mario_dqn/mario_dqn_main.py
+++ b/mario_dqn/mario_dqn_main.py
@@ -1,140 +1,140 @@
-"""
-智能体训练入口，包含训练逻辑
-"""
-from tensorboardX import SummaryWriter
-from ding.config import compile_config
-from ding.worker import BaseLearner, SampleSerialCollector, InteractionSerialEvaluator, AdvancedReplayBuffer
-from ding.envs import SyncSubprocessEnvManager, DingEnvWrapper, BaseEnvManager
-from wrapper import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
-    FinalEvalRewardEnv
-from policy import DQNPolicy
-from model import DQN
-from ding.utils import set_pkg_seed
-from ding.rl_utils import get_epsilon_greedy_fn
-from mario_dqn_config import mario_dqn_config
-from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
-from nes_py.wrappers import JoypadSpace
-from functools import partial
-import os
-import gym_super_mario_bros
-
-
-# 动作相关配置
-action_dict = {2: [["right"], ["right", "A"]], 7: SIMPLE_MOVEMENT, 12: COMPLEX_MOVEMENT}
-action_nums = [2, 7, 12]
-
-
-# mario环境
-def wrapped_mario_env(version=0, action=7, obs=1):
-    return DingEnvWrapper(
-        # 设置mario游戏版本与动作空间
-        JoypadSpace(gym_super_mario_bros.make("SuperMarioBros-1-1-v"+str(version)), action_dict[int(action)]),
-        cfg={
-            # 添加各种wrapper
-            'env_wrapper': [
-                # 默认wrapper：跳帧以降低计算量
-                lambda env: MaxAndSkipWrapper(env, skip=4),
-                # 默认wrapper：将mario游戏环境图片进行处理，返回大小为84X84的图片observation
-                lambda env: WarpFrameWrapper(env, size=84),
-                # 默认wrapper：将observation数值进行归一化
-                lambda env: ScaledFloatFrameWrapper(env),
-                # 默认wrapper：叠帧，将连续n_frames帧叠到一起，返回shape为(n_frames,84,84)的图片observation
-                lambda env: FrameStackWrapper(env, n_frames=obs),
-                # 默认wrapper：在评估一局游戏结束时返回累计的奖励，方便统计
-                lambda env: FinalEvalRewardEnv(env),
-                # 以下是你添加的wrapper
-            ]
-        }
-    )
-
-
-def main(cfg, args, seed=0, max_env_step=int(3e6)):
-    # Easydict类实例，包含一些配置
-    cfg = compile_config(
-        cfg,
-        SyncSubprocessEnvManager,
-        DQNPolicy,
-        BaseLearner,
-        SampleSerialCollector,
-        InteractionSerialEvaluator,
-        AdvancedReplayBuffer,
-        seed=seed,
-        save_cfg=True
-    )
-    # 收集经验的环境数量以及用于评估的环境数量
-    collector_env_num, evaluator_env_num = cfg.env.collector_env_num, cfg.env.evaluator_env_num
-    # 收集经验的环境，使用并行环境管理器
-    collector_env = SyncSubprocessEnvManager(
-        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs) for _ in range(collector_env_num)], cfg=cfg.env.manager
-    )
-    # 评估性能的环境，使用并行环境管理器
-    evaluator_env = SyncSubprocessEnvManager(
-        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs) for _ in range(evaluator_env_num)], cfg=cfg.env.manager
-    )
-
-    # 为mario环境设置种子
-    collector_env.seed(seed)
-    evaluator_env.seed(seed, dynamic_seed=False)
-    # 为torch、numpy、random等package设置种子
-    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)
-
-    # 采用DQN模型
-    model = DQN(**cfg.policy.model)
-    # 采用DQN策略
-    policy = DQNPolicy(cfg.policy, model=model)
-
-    # 设置学习、经验收集、评估、经验回放等强化学习常用配置
-    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))
-    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name)
-    collector = SampleSerialCollector(
-        cfg.policy.collect.collector, collector_env, policy.collect_mode, tb_logger, exp_name=cfg.exp_name
-    )
-    evaluator = InteractionSerialEvaluator(
-        cfg.policy.eval.evaluator, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name
-    )
-    replay_buffer = AdvancedReplayBuffer(cfg.policy.other.replay_buffer, tb_logger, exp_name=cfg.exp_name)
-
-    # 设置epsilon greedy
-    eps_cfg = cfg.policy.other.eps
-    epsilon_greedy = get_epsilon_greedy_fn(eps_cfg.start, eps_cfg.end, eps_cfg.decay, eps_cfg.type)
-
-    # 训练以及评估
-    while True:
-        # 根据当前训练迭代数决定是否进行评估
-        if evaluator.should_eval(learner.train_iter):
-            stop, reward = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)
-            if stop:
-                break
-        # 更新epsilon greedy信息
-        eps = epsilon_greedy(collector.envstep)
-        # 经验收集器从环境中收集经验
-        new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs={'eps': eps})
-        # 将收集的经验放入replay buffer
-        replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)
-        # 采样经验进行训练
-        for i in range(cfg.policy.learn.update_per_collect):
-            train_data = replay_buffer.sample(learner.policy.get_attribute('batch_size'), learner.train_iter)
-            if train_data is None:
-                break
-            learner.train(train_data, collector.envstep)
-        if collector.envstep >= max_env_step:
-            break
-
-
-if __name__ == "__main__":
-    from copy import deepcopy
-    import argparse
-    parser = argparse.ArgumentParser()
-    # 种子
-    parser.add_argument("--seed", "-s", type=int, default=0)
-    # 游戏版本，v0 v1 v2 v3 四种选择
-    parser.add_argument("--version", "-v", type=int, default=0, choices=[0,1,2,3])
-    # 动作集合种类，包含[["right"], ["right", "A"]]、SIMPLE_MOVEMENT、COMPLEX_MOVEMENT，分别对应2、7、12个动作
-    parser.add_argument("--action", "-a", type=int, default=7, choices=[2,7,12])
-    # 观测空间叠帧数目，不叠帧或叠四帧
-    parser.add_argument("--obs", "-o", type=int, default=1, choices=[1,4])
-    args = parser.parse_args()
-    mario_dqn_config.exp_name = 'exp/v'+str(args.version)+'_'+str(args.action)+'a_'+str(args.obs)+'f_seed'+str(args.seed)
-    mario_dqn_config.policy.model.obs_shape=[args.obs, 84, 84]
-    mario_dqn_config.policy.model.action_shape=args.action
+"""
+智能体训练入口，包含训练逻辑
+"""
+from tensorboardX import SummaryWriter
+from ding.config import compile_config
+from ding.worker import BaseLearner, SampleSerialCollector, InteractionSerialEvaluator, AdvancedReplayBuffer
+from ding.envs import SyncSubprocessEnvManager, DingEnvWrapper, BaseEnvManager
+from wrapper import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
+    FinalEvalRewardEnv
+from policy import DQNPolicy
+from model import DQN
+from ding.utils import set_pkg_seed
+from ding.rl_utils import get_epsilon_greedy_fn
+from mario_dqn_config import mario_dqn_config
+from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
+from nes_py.wrappers import JoypadSpace
+from functools import partial
+import os
+import gym_super_mario_bros
+
+
+# 动作相关配置
+action_dict = {2: [["right"], ["right", "A"]], 4: [["right"], ["right", "A"], ["left"], ["left", "A"]], 7: SIMPLE_MOVEMENT, 12: COMPLEX_MOVEMENT}
+action_nums = [2, 7, 12]
+
+
+# mario环境
+def wrapped_mario_env(version=0, action=7, obs=1):
+    return DingEnvWrapper(
+        # 设置mario游戏版本与动作空间
+        JoypadSpace(gym_super_mario_bros.make("SuperMarioBros-1-1-v"+str(version)), action_dict[int(action)]),
+        cfg={
+            # 添加各种wrapper
+            'env_wrapper': [
+                # 默认wrapper：跳帧以降低计算量
+                lambda env: MaxAndSkipWrapper(env, skip=4),
+                # 默认wrapper：将mario游戏环境图片进行处理，返回大小为84X84的图片observation
+                lambda env: WarpFrameWrapper(env, size=84),
+                # 默认wrapper：将observation数值进行归一化
+                lambda env: ScaledFloatFrameWrapper(env),
+                # 默认wrapper：叠帧，将连续n_frames帧叠到一起，返回shape为(n_frames,84,84)的图片observation
+                lambda env: FrameStackWrapper(env, n_frames=obs),
+                # 默认wrapper：在评估一局游戏结束时返回累计的奖励，方便统计
+                lambda env: FinalEvalRewardEnv(env),
+                # 以下是你添加的wrapper
+            ]
+        }
+    )
+
+
+def main(cfg, args, seed=0, max_env_step=int(3e6)):
+    # Easydict类实例，包含一些配置
+    cfg = compile_config(
+        cfg,
+        SyncSubprocessEnvManager,
+        DQNPolicy,
+        BaseLearner,
+        SampleSerialCollector,
+        InteractionSerialEvaluator,
+        AdvancedReplayBuffer,
+        seed=seed,
+        save_cfg=True
+    )
+    # 收集经验的环境数量以及用于评估的环境数量
+    collector_env_num, evaluator_env_num = cfg.env.collector_env_num, cfg.env.evaluator_env_num
+    # 收集经验的环境，使用并行环境管理器
+    collector_env = SyncSubprocessEnvManager(
+        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs) for _ in range(collector_env_num)], cfg=cfg.env.manager
+    )
+    # 评估性能的环境，使用并行环境管理器
+    evaluator_env = SyncSubprocessEnvManager(
+        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs) for _ in range(evaluator_env_num)], cfg=cfg.env.manager
+    )
+
+    # 为mario环境设置种子
+    collector_env.seed(seed)
+    evaluator_env.seed(seed, dynamic_seed=False)
+    # 为torch、numpy、random等package设置种子
+    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)
+
+    # 采用DQN模型
+    model = DQN(**cfg.policy.model)
+    # 采用DQN策略
+    policy = DQNPolicy(cfg.policy, model=model)
+
+    # 设置学习、经验收集、评估、经验回放等强化学习常用配置
+    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))
+    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name)
+    collector = SampleSerialCollector(
+        cfg.policy.collect.collector, collector_env, policy.collect_mode, tb_logger, exp_name=cfg.exp_name
+    )
+    evaluator = InteractionSerialEvaluator(
+        cfg.policy.eval.evaluator, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name
+    )
+    replay_buffer = AdvancedReplayBuffer(cfg.policy.other.replay_buffer, tb_logger, exp_name=cfg.exp_name)
+
+    # 设置epsilon greedy
+    eps_cfg = cfg.policy.other.eps
+    epsilon_greedy = get_epsilon_greedy_fn(eps_cfg.start, eps_cfg.end, eps_cfg.decay, eps_cfg.type)
+
+    # 训练以及评估
+    while True:
+        # 根据当前训练迭代数决定是否进行评估
+        if evaluator.should_eval(learner.train_iter):
+            stop, reward = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)
+            if stop:
+                break
+        # 更新epsilon greedy信息
+        eps = epsilon_greedy(collector.envstep)
+        # 经验收集器从环境中收集经验
+        new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs={'eps': eps})
+        # 将收集的经验放入replay buffer
+        replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)
+        # 采样经验进行训练
+        for i in range(cfg.policy.learn.update_per_collect):
+            train_data = replay_buffer.sample(learner.policy.get_attribute('batch_size'), learner.train_iter)
+            if train_data is None:
+                break
+            learner.train(train_data, collector.envstep)
+        if collector.envstep >= max_env_step:
+            break
+
+
+if __name__ == "__main__":
+    from copy import deepcopy
+    import argparse
+    parser = argparse.ArgumentParser()
+    # 种子
+    parser.add_argument("--seed", "-s", type=int, default=0)
+    # 游戏版本，v0 v1 v2 v3 四种选择
+    parser.add_argument("--version", "-v", type=int, default=0, choices=[0,1,2,3])
+    # 动作集合种类，包含[["right"], ["right", "A"]]、SIMPLE_MOVEMENT、COMPLEX_MOVEMENT，分别对应2、7、12个动作
+    parser.add_argument("--action", "-a", type=int, default=7, choices=[2,7,4,12])
+    # 观测空间叠帧数目，不叠帧或叠四帧
+    parser.add_argument("--obs", "-o", type=int, default=1, choices=[1,4, 8])
+    args = parser.parse_args()
+    mario_dqn_config.exp_name = 'exp/v'+str(args.version)+'_'+str(args.action)+'a_'+str(args.obs)+'f_seed'+str(args.seed)
+    mario_dqn_config.policy.model.obs_shape=[args.obs, 84, 84]
+    mario_dqn_config.policy.model.action_shape=args.action
     main(deepcopy(mario_dqn_config), args, seed=args.seed)
\ No newline at end of file
diff --git a/mario_dqn/model.py b/mario_dqn/model.py
index c9ef007..3d28791 100644
--- a/mario_dqn/model.py
+++ b/mario_dqn/model.py
@@ -1,111 +1,111 @@
-"""
-神经网络模型定义
-"""
-from typing import Union, Optional, Dict, Callable, List
-import torch
-import torch.nn as nn
-
-from ding.utils import SequenceType, squeeze
-from ding.model.common import FCEncoder, ConvEncoder, DiscreteHead, DuelingHead, MultiHead
-
-
-class DQN(nn.Module):
-
-    mode = ['compute_q', 'compute_q_logit']
-
-    def __init__(
-            self,
-            obs_shape: Union[int, SequenceType],
-            action_shape: Union[int, SequenceType],
-            encoder_hidden_size_list: SequenceType = [128, 128, 64],
-            dueling: bool = True,
-            head_hidden_size: Optional[int] = None,
-            head_layer_num: int = 1,
-            activation: Optional[nn.Module] = nn.ReLU(),
-            norm_type: Optional[str] = None
-    ) -> None:
-        """
-        Overview:
-            Init the DQN (encoder + head) Model according to input arguments.
-        Arguments:
-            - obs_shape (:obj:`Union[int, SequenceType]`): Observation space shape, such as 8 or [4, 84, 84].
-            - action_shape (:obj:`Union[int, SequenceType]`): Action space shape, such as 6 or [2, 3, 3].
-            - encoder_hidden_size_list (:obj:`SequenceType`): Collection of ``hidden_size`` to pass to ``Encoder``, \
-                the last element must match ``head_hidden_size``.
-            - dueling (:obj:`dueling`): Whether choose ``DuelingHead`` or ``DiscreteHead(default)``.
-            - head_hidden_size (:obj:`Optional[int]`): The ``hidden_size`` of head network.
-            - head_layer_num (:obj:`int`): The number of layers used in the head network to compute Q value output
-            - activation (:obj:`Optional[nn.Module]`): The type of activation function in networks \
-                if ``None`` then default set it to ``nn.ReLU()``
-            - norm_type (:obj:`Optional[str]`): The type of normalization in networks, see \
-                ``ding.torch_utils.fc_block`` for more details.
-        """
-        super(DQN, self).__init__()
-        # For compatibility: 1, (1, ), [4, 32, 32]
-        obs_shape, action_shape = squeeze(obs_shape), squeeze(action_shape)
-        if head_hidden_size is None:
-            head_hidden_size = encoder_hidden_size_list[-1]
-        # FC Encoder
-        if isinstance(obs_shape, int) or len(obs_shape) == 1:
-            self.encoder = FCEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
-        # Conv Encoder
-        elif len(obs_shape) == 3:
-            self.encoder = ConvEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
-        else:
-            raise RuntimeError(
-                "not support obs_shape for pre-defined encoder: {}, please customize your own DQN".format(obs_shape)
-            )
-        # Head Type
-        if dueling:
-            head_cls = DuelingHead
-        else:
-            head_cls = DiscreteHead
-        multi_head = not isinstance(action_shape, int)
-        if multi_head:
-            self.head = MultiHead(
-                head_cls,
-                head_hidden_size,
-                action_shape,
-                layer_num=head_layer_num,
-                activation=activation,
-                norm_type=norm_type
-            )
-        else:
-            self.head = head_cls(
-                head_hidden_size, action_shape, head_layer_num, activation=activation, norm_type=norm_type
-            )
-
-
-    def forward(self, x: torch.Tensor, mode: str='compute_q_logit') -> Dict:
-        assert mode in self.mode, "not support forward mode: {}/{}".format(mode, self.mode)
-        return getattr(self, mode)(x)
-
-
-    def compute_q(self, x: torch.Tensor) -> Dict:
-        r"""
-        Overview:
-            DQN forward computation graph, input observation tensor to predict q_value.
-        Arguments:
-            - x (:obj:`torch.Tensor`): Observation inputs
-        Returns:
-            - outputs (:obj:`Dict`): DQN forward outputs, such as q_value.
-        ReturnsKeys:
-            - logit (:obj:`torch.Tensor`): Discrete Q-value output of each action dimension.
-        Shapes:
-            - x (:obj:`torch.Tensor`): :math:`(B, N)`, where B is batch size and N is ``obs_shape``
-            - logit (:obj:`torch.FloatTensor`): :math:`(B, M)`, where B is batch size and M is ``action_shape``
-        Examples:
-            >>> model = DQN(32, 6)  # arguments: 'obs_shape' and 'action_shape'
-            >>> inputs = torch.randn(4, 32)
-            >>> outputs = model(inputs)
-            >>> assert isinstance(outputs, dict) and outputs['logit'].shape == torch.Size([4, 6])
-        """
-        x = self.encoder(x)
-        x = self.head(x)
-        return x
-
-
-    def compute_q_logit(self, x: torch.Tensor) -> Dict:
-        x = self.encoder(x)
-        x = self.head(x)
+"""
+神经网络模型定义
+"""
+from typing import Union, Optional, Dict, Callable, List
+import torch
+import torch.nn as nn
+
+from ding.utils import SequenceType, squeeze
+from ding.model.common import FCEncoder, ConvEncoder, DiscreteHead, DuelingHead, MultiHead
+
+
+class DQN(nn.Module):
+
+    mode = ['compute_q', 'compute_q_logit']
+
+    def __init__(
+            self,
+            obs_shape: Union[int, SequenceType],
+            action_shape: Union[int, SequenceType],
+            encoder_hidden_size_list: SequenceType = [128, 128, 64],
+            dueling: bool = True,
+            head_hidden_size: Optional[int] = None,
+            head_layer_num: int = 1,
+            activation: Optional[nn.Module] = nn.ReLU(),
+            norm_type: Optional[str] = None
+    ) -> None:
+        """
+        Overview:
+            Init the DQN (encoder + head) Model according to input arguments.
+        Arguments:
+            - obs_shape (:obj:`Union[int, SequenceType]`): Observation space shape, such as 8 or [4, 84, 84].
+            - action_shape (:obj:`Union[int, SequenceType]`): Action space shape, such as 6 or [2, 3, 3].
+            - encoder_hidden_size_list (:obj:`SequenceType`): Collection of ``hidden_size`` to pass to ``Encoder``, \
+                the last element must match ``head_hidden_size``.
+            - dueling (:obj:`dueling`): Whether choose ``DuelingHead`` or ``DiscreteHead(default)``.
+            - head_hidden_size (:obj:`Optional[int]`): The ``hidden_size`` of head network.
+            - head_layer_num (:obj:`int`): The number of layers used in the head network to compute Q value output
+            - activation (:obj:`Optional[nn.Module]`): The type of activation function in networks \
+                if ``None`` then default set it to ``nn.ReLU()``
+            - norm_type (:obj:`Optional[str]`): The type of normalization in networks, see \
+                ``ding.torch_utils.fc_block`` for more details.
+        """
+        super(DQN, self).__init__()
+        # For compatibility: 1, (1, ), [4, 32, 32]
+        obs_shape, action_shape = squeeze(obs_shape), squeeze(action_shape)
+        if head_hidden_size is None:
+            head_hidden_size = encoder_hidden_size_list[-1]
+        # FC Encoder
+        if isinstance(obs_shape, int) or len(obs_shape) == 1:
+            self.encoder = FCEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
+        # Conv Encoder
+        elif len(obs_shape) == 3:
+            self.encoder = ConvEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
+        else:
+            raise RuntimeError(
+                "not support obs_shape for pre-defined encoder: {}, please customize your own DQN".format(obs_shape)
+            )
+        # Head Type
+        if dueling:
+            head_cls = DuelingHead
+        else:
+            head_cls = DiscreteHead
+        multi_head = not isinstance(action_shape, int)
+        if multi_head:
+            self.head = MultiHead(
+                head_cls,
+                head_hidden_size,
+                action_shape,
+                layer_num=head_layer_num,
+                activation=activation,
+                norm_type=norm_type
+            )
+        else:
+            self.head = head_cls(
+                head_hidden_size, action_shape, head_layer_num, activation=activation, norm_type=norm_type
+            )
+
+
+    def forward(self, x: torch.Tensor, mode: str='compute_q_logit') -> Dict:
+        assert mode in self.mode, "not support forward mode: {}/{}".format(mode, self.mode)
+        return getattr(self, mode)(x)
+
+
+    def compute_q(self, x: torch.Tensor) -> Dict:
+        r"""
+        Overview:
+            DQN forward computation graph, input observation tensor to predict q_value.
+        Arguments:
+            - x (:obj:`torch.Tensor`): Observation inputs
+        Returns:
+            - outputs (:obj:`Dict`): DQN forward outputs, such as q_value.
+        ReturnsKeys:
+            - logit (:obj:`torch.Tensor`): Discrete Q-value output of each action dimension.
+        Shapes:
+            - x (:obj:`torch.Tensor`): :math:`(B, N)`, where B is batch size and N is ``obs_shape``
+            - logit (:obj:`torch.FloatTensor`): :math:`(B, M)`, where B is batch size and M is ``action_shape``
+        Examples:
+            >>> model = DQN(32, 6)  # arguments: 'obs_shape' and 'action_shape'
+            >>> inputs = torch.randn(4, 32)
+            >>> outputs = model(inputs)
+            >>> assert isinstance(outputs, dict) and outputs['logit'].shape == torch.Size([4, 6])
+        """
+        x = self.encoder(x)
+        x = self.head(x)
+        return x
+
+
+    def compute_q_logit(self, x: torch.Tensor) -> Dict:
+        x = self.encoder(x)
+        x = self.head(x)
         return x['logit']
\ No newline at end of file
diff --git a/mario_dqn/policy.py b/mario_dqn/policy.py
index bd1e227..7b25021 100644
--- a/mario_dqn/policy.py
+++ b/mario_dqn/policy.py
@@ -1,386 +1,386 @@
-"""
-DQN算法
-"""
-from typing import List, Dict, Any, Tuple
-from collections import namedtuple
-import copy
-import torch
-
-from ding.torch_utils import Adam, to_device
-from ding.rl_utils import q_nstep_td_data, q_nstep_td_error, get_nstep_return_data, get_train_sample
-from ding.model import model_wrap
-from ding.utils.data import default_collate, default_decollate
-
-from ding.policy import Policy
-from ding.policy.common_utils import default_preprocess_learn
-
-
-class DQNPolicy(Policy):
-    r"""
-    Overview:
-        Policy class of DQN algorithm, extended by Double DQN/Dueling DQN/PER/multi-step TD.
-
-    Config:
-        == ==================== ======== ============== ======================================== =======================
-        ID Symbol               Type     Default Value  Description                              Other(Shape)
-        == ==================== ======== ============== ======================================== =======================
-        1  ``type``             str      dqn            | RL policy register name, refer to      | This arg is optional,
-                                                        | registry ``POLICY_REGISTRY``           | a placeholder
-        2  ``cuda``             bool     False          | Whether to use cuda for network        | This arg can be diff-
-                                                                                                 | erent from modes
-        3  ``on_policy``        bool     False          | Whether the RL algorithm is on-policy
-                                                        | or off-policy
-        4  ``priority``         bool     False          | Whether use priority(PER)              | Priority sample,
-                                                                                                 | update priority
-        5  | ``priority_IS``    bool     False          | Whether use Importance Sampling Weight
-           | ``_weight``                                | to correct biased update. If True,
-                                                        | priority must be True.
-        6  | ``discount_``      float    0.97,          | Reward's future discount factor, aka.  | May be 1 when sparse
-           | ``factor``                  [0.95, 0.999]  | gamma                                  | reward env
-        7  ``nstep``            int      1,             | N-step reward discount sum for target
-                                         [3, 5]         | q_value estimation
-        8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
-           | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
-                                                        | valid in serial training               | means more off-policy
-        9  | ``learn.multi``    bool     False          | whether to use multi gpu during
-           | ``_gpu``
-        10 | ``learn.batch_``   int      64             | The number of samples of an iteration
-           | ``size``
-        11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
-           | ``_rate``
-        12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
-           | ``update_freq``
-        13 | ``learn.ignore_``  bool     False          | Whether ignore done for target value   | Enable it for some
-           | ``done``                                   | calculation.                           | fake termination env
-        14 ``collect.n_sample`` int      [8, 128]       | The number of training samples of a    | It varies from
-                                                        | call of collector.                     | different envs
-        15 | ``collect.unroll`` int      1              | unroll length of an iteration          | In RNN, unroll_len>1
-           | ``_len``
-        16 | ``other.eps.type`` str      exp            | exploration rate decay type            | Support ['exp',
-                                                                                                 | 'linear'].
-        17 | ``other.eps.``     float    0.95           | start value of exploration rate        | [0,1]
-           | ``start``
-        18 | ``other.eps.``     float    0.1            | end value of exploration rate          | [0,1]
-           | ``end``
-        19 | ``other.eps.``     int      10000          | decay length of exploration            | greater than 0. set
-           | ``decay``                                                                           | decay=10000 means
-                                                                                                 | the exploration rate
-                                                                                                 | decay from start
-                                                                                                 | value to end value
-                                                                                                 | during decay length.
-        == ==================== ======== ============== ======================================== =======================
-    """
-
-    config = dict(
-        type='dqn',
-        # (bool) Whether use cuda in policy
-        cuda=False,
-        # (bool) Whether learning policy is the same as collecting data policy(on-policy)
-        on_policy=False,
-        # (bool) Whether enable priority experience sample
-        priority=False,
-        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
-        priority_IS_weight=False,
-        # (float) Discount factor(gamma) for returns
-        discount_factor=0.97,
-        # (int) The number of step for calculating target q_value
-        nstep=1,
-        learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
-            # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
-            update_per_collect=3,
-            # (int) How many samples in a training batch
-            batch_size=64,
-            # (float) The step size of gradient descent
-            learning_rate=0.001,
-            # ==============================================================
-            # The following configs are algorithm-specific
-            # ==============================================================
-            # (int) Frequence of target network update.
-            target_update_freq=100,
-            # (bool) Whether ignore done(usually for max step termination env)
-            ignore_done=False,
-        ),
-        # collect_mode config
-        collect=dict(
-            # (int) Only one of [n_sample, n_episode] shoule be set
-            # n_sample=8,
-            # (int) Cut trajectories into pieces with length "unroll_len".
-            unroll_len=1,
-        ),
-        eval=dict(),
-        # other config
-        other=dict(
-            # Epsilon greedy with decay.
-            eps=dict(
-                # (str) Decay type. Support ['exp', 'linear'].
-                type='exp',
-                # (float) Epsilon start value
-                start=0.95,
-                # (float) Epsilon end value
-                end=0.1,
-                # (int) Decay length(env step)
-                decay=10000,
-            ),
-            replay_buffer=dict(replay_buffer_size=10000, ),
-        ),
-    )
-
-    def default_model(self) -> Tuple[str, List[str]]:
-        """
-        Overview:
-            Return this algorithm default model setting for demonstration.
-        Returns:
-            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names
-
-        .. note::
-            The user can define and use customized network model but must obey the same inferface definition indicated \
-            by import_names path. For DQN, ``ding.model.template.q_learning.DQN``
-        """
-        return 'dqn', ['ding.model.template.q_learning']
-
-    def _init_learn(self) -> None:
-        """
-        Overview:
-            Learn mode init method. Called by ``self.__init__``, initialize the optimizer, algorithm arguments, main \
-            and target model.
-        """
-        self._priority = self._cfg.priority
-        self._priority_IS_weight = self._cfg.priority_IS_weight
-        # Optimizer
-        self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate)
-
-        self._gamma = self._cfg.discount_factor
-        self._nstep = self._cfg.nstep
-
-        # use model_wrapper for specialized demands of different modes
-        self._target_model = copy.deepcopy(self._model)
-        self._target_model = model_wrap(
-            self._target_model,
-            wrapper_name='target',
-            update_type='assign',
-            update_kwargs={'freq': self._cfg.learn.target_update_freq}
-        )
-        self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')
-        self._learn_model.reset()
-        self._target_model.reset()
-
-    def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:
-        """
-        Overview:
-            Forward computation graph of learn mode(updating policy).
-        Arguments:
-            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \
-                np.ndarray or dict/list combinations.
-        Returns:
-            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \
-                recorded in text log and tensorboard, values are python scalar or a list of scalars.
-        ArgumentsKeys:
-            - necessary: ``obs``, ``action``, ``reward``, ``next_obs``, ``done``
-            - optional: ``value_gamma``, ``IS``
-        ReturnsKeys:
-            - necessary: ``cur_lr``, ``total_loss``, ``priority``
-            - optional: ``action_distribution``
-        """
-        data = default_preprocess_learn(
-            data,
-            use_priority=self._priority,
-            use_priority_IS_weight=self._cfg.priority_IS_weight,
-            ignore_done=self._cfg.learn.ignore_done,
-            use_nstep=True
-        )
-        if self._cuda:
-            data = to_device(data, self._device)
-        # ====================
-        # Q-learning forward
-        # ====================
-        self._learn_model.train()
-        self._target_model.train()
-        # Current q value (main model)
-        q_value = self._learn_model.forward(data['obs'], mode='compute_q')['logit']
-        # Target q value
-        with torch.no_grad():
-            target_q_value = self._target_model.forward(data['next_obs'], mode='compute_q')['logit']
-            # Max q value action (main model)
-            target_q_action = self._learn_model.forward(data['next_obs'], mode='compute_q')['action']
-
-        data_n = q_nstep_td_data(
-            q_value, target_q_value, data['action'], target_q_action, data['reward'], data['done'], data['weight']
-        )
-        value_gamma = data.get('value_gamma')
-        loss, td_error_per_sample = q_nstep_td_error(data_n, self._gamma, nstep=self._nstep, value_gamma=value_gamma)
-
-        # ====================
-        # Q-learning update
-        # ====================
-        self._optimizer.zero_grad()
-        loss.backward()
-        if self._cfg.learn.multi_gpu:
-            self.sync_gradients(self._learn_model)
-        self._optimizer.step()
-
-        # =============
-        # after update
-        # =============
-        self._target_model.update(self._learn_model.state_dict())
-        return {
-            'cur_lr': self._optimizer.defaults['lr'],
-            'total_loss': loss.item(),
-            'q_value': q_value.mean().item(),
-            'target_q_value': target_q_value.mean().item(),
-            'priority': td_error_per_sample.abs().tolist(),
-            # Only discrete action satisfying len(data['action'])==1 can return this and draw histogram on tensorboard.
-            # '[histogram]action_distribution': data['action'],
-        }
-
-    def _monitor_vars_learn(self) -> List[str]:
-        return ['cur_lr', 'total_loss', 'q_value', 'target_q_value']
-
-    def _state_dict_learn(self) -> Dict[str, Any]:
-        """
-        Overview:
-            Return the state_dict of learn mode, usually including model and optimizer.
-        Returns:
-            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.
-        """
-        return {
-            'model': self._learn_model.state_dict(),
-            'target_model': self._target_model.state_dict(),
-            'optimizer': self._optimizer.state_dict(),
-        }
-
-    def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:
-        """
-        Overview:
-            Load the state_dict variable into policy learn mode.
-        Arguments:
-            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.
-
-        .. tip::
-            If you want to only load some parts of model, you can simply set the ``strict`` argument in \
-            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \
-            complicated operation.
-        """
-        self._learn_model.load_state_dict(state_dict['model'])
-        self._target_model.load_state_dict(state_dict['target_model'])
-        self._optimizer.load_state_dict(state_dict['optimizer'])
-
-    def _init_collect(self) -> None:
-        """
-        Overview:
-            Collect mode init method. Called by ``self.__init__``, initialize algorithm arguments and collect_model, \
-            enable the eps_greedy_sample for exploration.
-        """
-        self._unroll_len = self._cfg.collect.unroll_len
-        self._gamma = self._cfg.discount_factor  # necessary for parallel
-        self._nstep = self._cfg.nstep  # necessary for parallel
-        self._collect_model = model_wrap(self._model, wrapper_name='eps_greedy_sample')
-        self._collect_model.reset()
-
-    def _forward_collect(self, data: Dict[int, Any], eps: float) -> Dict[int, Any]:
-        """
-        Overview:
-            Forward computation graph of collect mode(collect training data), with eps_greedy for exploration.
-        Arguments:
-            - data (:obj:`Dict[str, Any]`): Dict type data, stacked env data for predicting policy_output(action), \
-                values are torch.Tensor or np.ndarray or dict/list combinations, keys are env_id indicated by integer.
-            - eps (:obj:`float`): epsilon value for exploration, which is decayed by collected env step.
-        Returns:
-            - output (:obj:`Dict[int, Any]`): The dict of predicting policy_output(action) for the interaction with \
-                env and the constructing of transition.
-        ArgumentsKeys:
-            - necessary: ``obs``
-        ReturnsKeys
-            - necessary: ``logit``, ``action``
-        """
-        data_id = list(data.keys())
-        data = default_collate(list(data.values()))
-        if self._cuda:
-            data = to_device(data, self._device)
-        self._collect_model.eval()
-        with torch.no_grad():
-            output = self._collect_model.forward(data, mode='compute_q', eps=eps)
-        if self._cuda:
-            output = to_device(output, 'cpu')
-        output = default_decollate(output)
-        return {i: d for i, d in zip(data_id, output)}
-
-    def _get_train_sample(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
-        """
-        Overview:
-            For a given trajectory(transitions, a list of transition) data, process it into a list of sample that \
-            can be used for training directly. A train sample can be a processed transition(DQN with nstep TD) \
-            or some continuous transitions(DRQN).
-        Arguments:
-            - data (:obj:`List[Dict[str, Any]`): The trajectory data(a list of transition), each element is the same \
-                format as the return value of ``self._process_transition`` method.
-        Returns:
-            - samples (:obj:`dict`): The list of training samples.
-
-        .. note::
-            We will vectorize ``process_transition`` and ``get_train_sample`` method in the following release version. \
-            And the user can customize the this data processing procecure by overriding this two methods and collector \
-            itself.
-        """
-        data = get_nstep_return_data(data, self._nstep, gamma=self._gamma)
-        return get_train_sample(data, self._unroll_len)
-
-    def _process_transition(self, obs: Any, policy_output: Dict[str, Any], timestep: namedtuple) -> Dict[str, Any]:
-        """
-        Overview:
-            Generate a transition(e.g.: <s, a, s', r, d>) for this algorithm training.
-        Arguments:
-            - obs (:obj:`Any`): Env observation.
-            - policy_output (:obj:`Dict[str, Any]`): The output of policy collect mode(``self._forward_collect``),\
-                including at least ``action``.
-            - timestep (:obj:`namedtuple`): The output after env step(execute policy output action), including at \
-                least ``obs``, ``reward``, ``done``, (here obs indicates obs after env step).
-        Returns:
-            - transition (:obj:`dict`): Dict type transition data.
-        """
-        transition = {
-            'obs': obs,
-            'next_obs': timestep.obs,
-            'action': policy_output['action'],
-            'reward': timestep.reward,
-            'done': timestep.done,
-        }
-        return transition
-
-    def _init_eval(self) -> None:
-        r"""
-        Overview:
-            Evaluate mode init method. Called by ``self.__init__``, initialize eval_model.
-        """
-        self._eval_model = model_wrap(self._model, wrapper_name='argmax_sample')
-        self._eval_model.reset()
-
-    def _forward_eval(self, data: Dict[int, Any]) -> Dict[int, Any]:
-        """
-        Overview:
-            Forward computation graph of eval mode(evaluate policy performance), at most cases, it is similar to \
-            ``self._forward_collect``.
-        Arguments:
-            - data (:obj:`Dict[str, Any]`): Dict type data, stacked env data for predicting policy_output(action), \
-                values are torch.Tensor or np.ndarray or dict/list combinations, keys are env_id indicated by integer.
-        Returns:
-            - output (:obj:`Dict[int, Any]`): The dict of predicting action for the interaction with env.
-        ArgumentsKeys:
-            - necessary: ``obs``
-        ReturnsKeys
-            - necessary: ``action``
-        """
-        data_id = list(data.keys())
-        data = default_collate(list(data.values()))
-        if self._cuda:
-            data = to_device(data, self._device)
-        self._eval_model.eval()
-        with torch.no_grad():
-            output = self._eval_model.forward(data, mode='compute_q')
-        if self._cuda:
-            output = to_device(output, 'cpu')
-        output = default_decollate(output)
+"""
+DQN算法
+"""
+from typing import List, Dict, Any, Tuple
+from collections import namedtuple
+import copy
+import torch
+
+from ding.torch_utils import Adam, to_device
+from ding.rl_utils import q_nstep_td_data, q_nstep_td_error, get_nstep_return_data, get_train_sample
+from ding.model import model_wrap
+from ding.utils.data import default_collate, default_decollate
+
+from ding.policy import Policy
+from ding.policy.common_utils import default_preprocess_learn
+
+
+class DQNPolicy(Policy):
+    r"""
+    Overview:
+        Policy class of DQN algorithm, extended by Double DQN/Dueling DQN/PER/multi-step TD.
+
+    Config:
+        == ==================== ======== ============== ======================================== =======================
+        ID Symbol               Type     Default Value  Description                              Other(Shape)
+        == ==================== ======== ============== ======================================== =======================
+        1  ``type``             str      dqn            | RL policy register name, refer to      | This arg is optional,
+                                                        | registry ``POLICY_REGISTRY``           | a placeholder
+        2  ``cuda``             bool     False          | Whether to use cuda for network        | This arg can be diff-
+                                                                                                 | erent from modes
+        3  ``on_policy``        bool     False          | Whether the RL algorithm is on-policy
+                                                        | or off-policy
+        4  ``priority``         bool     False          | Whether use priority(PER)              | Priority sample,
+                                                                                                 | update priority
+        5  | ``priority_IS``    bool     False          | Whether use Importance Sampling Weight
+           | ``_weight``                                | to correct biased update. If True,
+                                                        | priority must be True.
+        6  | ``discount_``      float    0.97,          | Reward's future discount factor, aka.  | May be 1 when sparse
+           | ``factor``                  [0.95, 0.999]  | gamma                                  | reward env
+        7  ``nstep``            int      1,             | N-step reward discount sum for target
+                                         [3, 5]         | q_value estimation
+        8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
+           | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
+                                                        | valid in serial training               | means more off-policy
+        9  | ``learn.multi``    bool     False          | whether to use multi gpu during
+           | ``_gpu``
+        10 | ``learn.batch_``   int      64             | The number of samples of an iteration
+           | ``size``
+        11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
+           | ``_rate``
+        12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
+           | ``update_freq``
+        13 | ``learn.ignore_``  bool     False          | Whether ignore done for target value   | Enable it for some
+           | ``done``                                   | calculation.                           | fake termination env
+        14 ``collect.n_sample`` int      [8, 128]       | The number of training samples of a    | It varies from
+                                                        | call of collector.                     | different envs
+        15 | ``collect.unroll`` int      1              | unroll length of an iteration          | In RNN, unroll_len>1
+           | ``_len``
+        16 | ``other.eps.type`` str      exp            | exploration rate decay type            | Support ['exp',
+                                                                                                 | 'linear'].
+        17 | ``other.eps.``     float    0.95           | start value of exploration rate        | [0,1]
+           | ``start``
+        18 | ``other.eps.``     float    0.1            | end value of exploration rate          | [0,1]
+           | ``end``
+        19 | ``other.eps.``     int      10000          | decay length of exploration            | greater than 0. set
+           | ``decay``                                                                           | decay=10000 means
+                                                                                                 | the exploration rate
+                                                                                                 | decay from start
+                                                                                                 | value to end value
+                                                                                                 | during decay length.
+        == ==================== ======== ============== ======================================== =======================
+    """
+
+    config = dict(
+        type='dqn',
+        # (bool) Whether use cuda in policy
+        cuda=False,
+        # (bool) Whether learning policy is the same as collecting data policy(on-policy)
+        on_policy=False,
+        # (bool) Whether enable priority experience sample
+        priority=False,
+        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
+        priority_IS_weight=False,
+        # (float) Discount factor(gamma) for returns
+        discount_factor=0.97,
+        # (int) The number of step for calculating target q_value
+        nstep=1,
+        learn=dict(
+            # (bool) Whether to use multi gpu
+            multi_gpu=False,
+            # How many updates(iterations) to train after collector's one collection.
+            # Bigger "update_per_collect" means bigger off-policy.
+            # collect data -> update policy-> collect data -> ...
+            update_per_collect=3,
+            # (int) How many samples in a training batch
+            batch_size=64,
+            # (float) The step size of gradient descent
+            learning_rate=0.001,
+            # ==============================================================
+            # The following configs are algorithm-specific
+            # ==============================================================
+            # (int) Frequence of target network update.
+            target_update_freq=100,
+            # (bool) Whether ignore done(usually for max step termination env)
+            ignore_done=False,
+        ),
+        # collect_mode config
+        collect=dict(
+            # (int) Only one of [n_sample, n_episode] shoule be set
+            # n_sample=8,
+            # (int) Cut trajectories into pieces with length "unroll_len".
+            unroll_len=1,
+        ),
+        eval=dict(),
+        # other config
+        other=dict(
+            # Epsilon greedy with decay.
+            eps=dict(
+                # (str) Decay type. Support ['exp', 'linear'].
+                type='exp',
+                # (float) Epsilon start value
+                start=0.95,
+                # (float) Epsilon end value
+                end=0.1,
+                # (int) Decay length(env step)
+                decay=10000,
+            ),
+            replay_buffer=dict(replay_buffer_size=10000, ),
+        ),
+    )
+
+    def default_model(self) -> Tuple[str, List[str]]:
+        """
+        Overview:
+            Return this algorithm default model setting for demonstration.
+        Returns:
+            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names
+
+        .. note::
+            The user can define and use customized network model but must obey the same inferface definition indicated \
+            by import_names path. For DQN, ``ding.model.template.q_learning.DQN``
+        """
+        return 'dqn', ['ding.model.template.q_learning']
+
+    def _init_learn(self) -> None:
+        """
+        Overview:
+            Learn mode init method. Called by ``self.__init__``, initialize the optimizer, algorithm arguments, main \
+            and target model.
+        """
+        self._priority = self._cfg.priority
+        self._priority_IS_weight = self._cfg.priority_IS_weight
+        # Optimizer
+        self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate)
+
+        self._gamma = self._cfg.discount_factor
+        self._nstep = self._cfg.nstep
+
+        # use model_wrapper for specialized demands of different modes
+        self._target_model = copy.deepcopy(self._model)
+        self._target_model = model_wrap(
+            self._target_model,
+            wrapper_name='target',
+            update_type='assign',
+            update_kwargs={'freq': self._cfg.learn.target_update_freq}
+        )
+        self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')
+        self._learn_model.reset()
+        self._target_model.reset()
+
+    def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Overview:
+            Forward computation graph of learn mode(updating policy).
+        Arguments:
+            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \
+                np.ndarray or dict/list combinations.
+        Returns:
+            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \
+                recorded in text log and tensorboard, values are python scalar or a list of scalars.
+        ArgumentsKeys:
+            - necessary: ``obs``, ``action``, ``reward``, ``next_obs``, ``done``
+            - optional: ``value_gamma``, ``IS``
+        ReturnsKeys:
+            - necessary: ``cur_lr``, ``total_loss``, ``priority``
+            - optional: ``action_distribution``
+        """
+        data = default_preprocess_learn(
+            data,
+            use_priority=self._priority,
+            use_priority_IS_weight=self._cfg.priority_IS_weight,
+            ignore_done=self._cfg.learn.ignore_done,
+            use_nstep=True
+        )
+        if self._cuda:
+            data = to_device(data, self._device)
+        # ====================
+        # Q-learning forward
+        # ====================
+        self._learn_model.train()
+        self._target_model.train()
+        # Current q value (main model)
+        q_value = self._learn_model.forward(data['obs'], mode='compute_q')['logit']
+        # Target q value
+        with torch.no_grad():
+            target_q_value = self._target_model.forward(data['next_obs'], mode='compute_q')['logit']
+            # Max q value action (main model)
+            target_q_action = self._learn_model.forward(data['next_obs'], mode='compute_q')['action']
+
+        data_n = q_nstep_td_data(
+            q_value, target_q_value, data['action'], target_q_action, data['reward'], data['done'], data['weight']
+        )
+        value_gamma = data.get('value_gamma')
+        loss, td_error_per_sample = q_nstep_td_error(data_n, self._gamma, nstep=self._nstep, value_gamma=value_gamma)
+
+        # ====================
+        # Q-learning update
+        # ====================
+        self._optimizer.zero_grad()
+        loss.backward()
+        if self._cfg.learn.multi_gpu:
+            self.sync_gradients(self._learn_model)
+        self._optimizer.step()
+
+        # =============
+        # after update
+        # =============
+        self._target_model.update(self._learn_model.state_dict())
+        return {
+            'cur_lr': self._optimizer.defaults['lr'],
+            'total_loss': loss.item(),
+            'q_value': q_value.mean().item(),
+            'target_q_value': target_q_value.mean().item(),
+            'priority': td_error_per_sample.abs().tolist(),
+            # Only discrete action satisfying len(data['action'])==1 can return this and draw histogram on tensorboard.
+            # '[histogram]action_distribution': data['action'],
+        }
+
+    def _monitor_vars_learn(self) -> List[str]:
+        return ['cur_lr', 'total_loss', 'q_value', 'target_q_value']
+
+    def _state_dict_learn(self) -> Dict[str, Any]:
+        """
+        Overview:
+            Return the state_dict of learn mode, usually including model and optimizer.
+        Returns:
+            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.
+        """
+        return {
+            'model': self._learn_model.state_dict(),
+            'target_model': self._target_model.state_dict(),
+            'optimizer': self._optimizer.state_dict(),
+        }
+
+    def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:
+        """
+        Overview:
+            Load the state_dict variable into policy learn mode.
+        Arguments:
+            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.
+
+        .. tip::
+            If you want to only load some parts of model, you can simply set the ``strict`` argument in \
+            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \
+            complicated operation.
+        """
+        self._learn_model.load_state_dict(state_dict['model'])
+        self._target_model.load_state_dict(state_dict['target_model'])
+        self._optimizer.load_state_dict(state_dict['optimizer'])
+
+    def _init_collect(self) -> None:
+        """
+        Overview:
+            Collect mode init method. Called by ``self.__init__``, initialize algorithm arguments and collect_model, \
+            enable the eps_greedy_sample for exploration.
+        """
+        self._unroll_len = self._cfg.collect.unroll_len
+        self._gamma = self._cfg.discount_factor  # necessary for parallel
+        self._nstep = self._cfg.nstep  # necessary for parallel
+        self._collect_model = model_wrap(self._model, wrapper_name='eps_greedy_sample')
+        self._collect_model.reset()
+
+    def _forward_collect(self, data: Dict[int, Any], eps: float) -> Dict[int, Any]:
+        """
+        Overview:
+            Forward computation graph of collect mode(collect training data), with eps_greedy for exploration.
+        Arguments:
+            - data (:obj:`Dict[str, Any]`): Dict type data, stacked env data for predicting policy_output(action), \
+                values are torch.Tensor or np.ndarray or dict/list combinations, keys are env_id indicated by integer.
+            - eps (:obj:`float`): epsilon value for exploration, which is decayed by collected env step.
+        Returns:
+            - output (:obj:`Dict[int, Any]`): The dict of predicting policy_output(action) for the interaction with \
+                env and the constructing of transition.
+        ArgumentsKeys:
+            - necessary: ``obs``
+        ReturnsKeys
+            - necessary: ``logit``, ``action``
+        """
+        data_id = list(data.keys())
+        data = default_collate(list(data.values()))
+        if self._cuda:
+            data = to_device(data, self._device)
+        self._collect_model.eval()
+        with torch.no_grad():
+            output = self._collect_model.forward(data, mode='compute_q', eps=eps)
+        if self._cuda:
+            output = to_device(output, 'cpu')
+        output = default_decollate(output)
+        return {i: d for i, d in zip(data_id, output)}
+
+    def _get_train_sample(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+        """
+        Overview:
+            For a given trajectory(transitions, a list of transition) data, process it into a list of sample that \
+            can be used for training directly. A train sample can be a processed transition(DQN with nstep TD) \
+            or some continuous transitions(DRQN).
+        Arguments:
+            - data (:obj:`List[Dict[str, Any]`): The trajectory data(a list of transition), each element is the same \
+                format as the return value of ``self._process_transition`` method.
+        Returns:
+            - samples (:obj:`dict`): The list of training samples.
+
+        .. note::
+            We will vectorize ``process_transition`` and ``get_train_sample`` method in the following release version. \
+            And the user can customize the this data processing procecure by overriding this two methods and collector \
+            itself.
+        """
+        data = get_nstep_return_data(data, self._nstep, gamma=self._gamma)
+        return get_train_sample(data, self._unroll_len)
+
+    def _process_transition(self, obs: Any, policy_output: Dict[str, Any], timestep: namedtuple) -> Dict[str, Any]:
+        """
+        Overview:
+            Generate a transition(e.g.: <s, a, s', r, d>) for this algorithm training.
+        Arguments:
+            - obs (:obj:`Any`): Env observation.
+            - policy_output (:obj:`Dict[str, Any]`): The output of policy collect mode(``self._forward_collect``),\
+                including at least ``action``.
+            - timestep (:obj:`namedtuple`): The output after env step(execute policy output action), including at \
+                least ``obs``, ``reward``, ``done``, (here obs indicates obs after env step).
+        Returns:
+            - transition (:obj:`dict`): Dict type transition data.
+        """
+        transition = {
+            'obs': obs,
+            'next_obs': timestep.obs,
+            'action': policy_output['action'],
+            'reward': timestep.reward,
+            'done': timestep.done,
+        }
+        return transition
+
+    def _init_eval(self) -> None:
+        r"""
+        Overview:
+            Evaluate mode init method. Called by ``self.__init__``, initialize eval_model.
+        """
+        self._eval_model = model_wrap(self._model, wrapper_name='argmax_sample')
+        self._eval_model.reset()
+
+    def _forward_eval(self, data: Dict[int, Any]) -> Dict[int, Any]:
+        """
+        Overview:
+            Forward computation graph of eval mode(evaluate policy performance), at most cases, it is similar to \
+            ``self._forward_collect``.
+        Arguments:
+            - data (:obj:`Dict[str, Any]`): Dict type data, stacked env data for predicting policy_output(action), \
+                values are torch.Tensor or np.ndarray or dict/list combinations, keys are env_id indicated by integer.
+        Returns:
+            - output (:obj:`Dict[int, Any]`): The dict of predicting action for the interaction with env.
+        ArgumentsKeys:
+            - necessary: ``obs``
+        ReturnsKeys
+            - necessary: ``action``
+        """
+        data_id = list(data.keys())
+        data = default_collate(list(data.values()))
+        if self._cuda:
+            data = to_device(data, self._device)
+        self._eval_model.eval()
+        with torch.no_grad():
+            output = self._eval_model.forward(data, mode='compute_q')
+        if self._cuda:
+            output = to_device(output, 'cpu')
+        output = default_decollate(output)
         return {i: d for i, d in zip(data_id, output)}
\ No newline at end of file
diff --git a/mario_dqn/requirements.txt b/mario_dqn/requirements.txt
index 60e91f6..b533f51 100644
--- a/mario_dqn/requirements.txt
+++ b/mario_dqn/requirements.txt
@@ -1,7 +1,7 @@
-torch==1.10.0
-git+http://github.com/opendilab/DI-engine@main
-gym-super-mario-bros==7.4.0
-gym==0.25.1
-opencv-python==4.6.0.66
-tensorboard==2.10.1
+torch==1.10.0
+git+http://github.com/opendilab/DI-engine@main
+gym-super-mario-bros==7.4.0
+gym==0.25.1
+opencv-python==4.6.0.66
+tensorboard==2.10.1
 grad-cam
\ No newline at end of file
diff --git a/mario_dqn/wrapper.py b/mario_dqn/wrapper.py
index b7b1a08..757c84e 100644
--- a/mario_dqn/wrapper.py
+++ b/mario_dqn/wrapper.py
@@ -1,242 +1,242 @@
-"""
-wrapper定义文件
-"""
-from typing import Union, List, Tuple, Callable
-from ding.envs.env_wrappers import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
-    FinalEvalRewardEnv
-import gym
-import numpy as np
-import cv2
-from pytorch_grad_cam import GradCAM
-import torch
-from ding.torch_utils import to_ndarray
-import os
-import warnings
-import copy
-
-
-# 粘性动作wrapper
-class StickyActionWrapper(gym.ActionWrapper):
-    """
-    Overview:
-       A certain possibility to select the last action
-    Interface:
-        ``__init__``, ``action``
-    Properties:
-        - env (:obj:`gym.Env`): the environment to wrap.
-        - ``p_sticky``: possibility to select the last action
-    """
-
-    def __init__(self, env: gym.Env, p_sticky: float=0.25):
-        super().__init__(env)
-        self.p_sticky = p_sticky
-        self.last_action = 0
-
-    def action(self, action):
-        if np.random.random() < self.p_sticky:
-            return_action = self.last_action
-        else:
-            return_action = action
-        self.last_action = action
-        return return_action
-
-
-# 稀疏奖励wrapper
-class SparseRewardWrapper(gym.Wrapper):
-    """
-    Overview:
-       Only death and pass sparse reward
-    Interface:
-        ``__init__``, ``step``
-    Properties:
-        - env (:obj:`gym.Env`): the environment to wrap.
-    """
-
-    def __init__(self, env: gym.Env):
-        super().__init__(env)
-
-    def step(self, action):
-        obs, reward, done, info = self.env.step(action)
-        dead = True if reward == -15 else False
-        reward = 0
-        if info['flag_get']:
-            reward = 15
-        if dead:
-            reward = -15
-        return obs, reward, done, info
-
-
-# 硬币奖励wrapper
-class CoinRewardWrapper(gym.Wrapper):
-    """
-    Overview:
-        add coin reward
-    Interface:
-        ``__init__``, ``step``
-    Properties:
-        - env (:obj:`gym.Env`): the environment to wrap.
-    """
-
-    def __init__(self, env: gym.Env):
-        super().__init__(env)
-        self.num_coins = 0
-
-    def step(self, action):
-        obs, reward, done, info = self.env.step(action)
-        reward += (info['coins'] - self.num_coins) * 10
-        self.num_coins = info['coins']
-        return obs, reward, done, info
-
-
-# CAM相关，不需要了解
-def dump_arr2video(arr, video_folder):
-    fourcc = cv2.VideoWriter_fourcc(*'MP4V')
-    fps = 6
-    size = (256, 240)
-    out = cv2.VideoWriter(video_folder + '/cam_pure.mp4', fourcc, fps, size)
-    out1 = cv2.VideoWriter(video_folder + '/obs_pure.mp4', fourcc, fps, size)
-    out2 = cv2.VideoWriter(video_folder + '/merged.mp4', fourcc, fps, size)
-    for frame, obs in arr:
-        frame = (255 * frame).astype('uint8').squeeze(0)
-        frame_c = cv2.resize(cv2.applyColorMap(frame, cv2.COLORMAP_JET), size)
-        out.write(frame_c)
-
-        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)
-        out1.write(obs)
-
-        merged_frame = cv2.addWeighted(obs, 0.6, frame_c, 0.4, 0)
-        out2.write(merged_frame)
-    # assert False
-
-
-def get_cam(img, model):
-    target_layers = [model.encoder.main[0]]
-    input_tensor = torch.from_numpy(img).unsqueeze(0)
-
-    # Construct the CAM object once, and then re-use it on many images:
-    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)
-    targets = None
-
-    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.
-    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)
-
-    # In this example grayscale_cam has only one image in the batch:
-    return grayscale_cam
-
-
-def capped_cubic_video_schedule(episode_id):
-    if episode_id < 1000:
-        return int(round(episode_id ** (1.0 / 3))) ** 3 == episode_id
-    else:
-        return episode_id % 1000 == 0
-
-
-class RecordCAM(gym.Wrapper):
-
-    def __init__(
-        self,
-        env,
-        cam_model,
-        video_folder: str,
-        episode_trigger: Callable[[int], bool] = None,
-        step_trigger: Callable[[int], bool] = None,
-        video_length: int = 0,
-        name_prefix: str = "rl-video",
-    ):
-        super(RecordCAM, self).__init__(env)
-        self._env = env
-        self.cam_model = cam_model
-
-        if episode_trigger is None and step_trigger is None:
-            episode_trigger = capped_cubic_video_schedule
-
-        trigger_count = sum([x is not None for x in [episode_trigger, step_trigger]])
-        assert trigger_count == 1, "Must specify exactly one trigger"
-
-        self.episode_trigger = episode_trigger
-        self.step_trigger = step_trigger
-        self.video_recorder = []
-
-        self.video_folder = os.path.abspath(video_folder)
-        # Create output folder if needed
-        if os.path.isdir(self.video_folder):
-            warnings.warn(
-                f"Overwriting existing videos at {self.video_folder} folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)"
-            )
-        os.makedirs(self.video_folder, exist_ok=True)
-
-        self.name_prefix = name_prefix
-        self.step_id = 0
-        self.video_length = video_length
-
-        self.recording = False
-        self.recorded_frames = 0
-        self.is_vector_env = getattr(env, "is_vector_env", False)
-        self.episode_id = 0
-
-    def reset(self, **kwargs):
-        observations = super(RecordCAM, self).reset(**kwargs)
-        if not self.recording:
-            self.start_video_recorder()
-        return observations
-
-    def start_video_recorder(self):
-        self.close_video_recorder()
-
-        video_name = f"{self.name_prefix}-step-{self.step_id}"
-        if self.episode_trigger:
-            video_name = f"{self.name_prefix}-episode-{self.episode_id}"
-
-        base_path = os.path.join(self.video_folder, video_name)
-        self.video_recorder = []
-
-        self.recorded_frames = 0
-        self.recording = True
-
-    def _video_enabled(self):
-        if self.step_trigger:
-            return self.step_trigger(self.step_id)
-        else:
-            return self.episode_trigger(self.episode_id)
-
-    def step(self, action):
-        time_step = super(RecordCAM, self).step(action)
-        observations, rewards, dones, infos = time_step
-
-        # increment steps and episodes
-        self.step_id += 1
-        if not self.is_vector_env:
-            if dones:
-                self.episode_id += 1
-        elif dones[0]:
-            self.episode_id += 1
-
-        if self.recording:
-            self.video_recorder.append(
-                (get_cam(observations, model=self.cam_model), copy.deepcopy(self.env.render(mode='rgb_array')))
-            )
-            self.recorded_frames += 1
-            if self.video_length > 0:
-                if self.recorded_frames > 10000:
-                    self.close_video_recorder()
-            else:
-                if not self.is_vector_env:
-                    if dones or infos['time'] < 250:
-                        self.close_video_recorder()
-                elif dones[0]:
-                    self.close_video_recorder()
-
-        elif self._video_enabled():
-            self.start_video_recorder()
-
-        return time_step
-
-    def close_video_recorder(self) -> None:
-        if self.recorded_frames > 0:
-            dump_arr2video(self.video_recorder, self.video_folder)
-        self.video_recorder = []
-        self.recording = False
-        self.recorded_frames = 0
-
-    def seed(self, seed: int) -> None:
+"""
+wrapper定义文件
+"""
+from typing import Union, List, Tuple, Callable
+from ding.envs.env_wrappers import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
+    FinalEvalRewardEnv
+import gym
+import numpy as np
+import cv2
+from pytorch_grad_cam import GradCAM
+import torch
+from ding.torch_utils import to_ndarray
+import os
+import warnings
+import copy
+
+
+# 粘性动作wrapper
+class StickyActionWrapper(gym.ActionWrapper):
+    """
+    Overview:
+       A certain possibility to select the last action
+    Interface:
+        ``__init__``, ``action``
+    Properties:
+        - env (:obj:`gym.Env`): the environment to wrap.
+        - ``p_sticky``: possibility to select the last action
+    """
+
+    def __init__(self, env: gym.Env, p_sticky: float=0.25):
+        super().__init__(env)
+        self.p_sticky = p_sticky
+        self.last_action = 0
+
+    def action(self, action):
+        if np.random.random() < self.p_sticky:
+            return_action = self.last_action
+        else:
+            return_action = action
+        self.last_action = action
+        return return_action
+
+
+# 稀疏奖励wrapper
+class SparseRewardWrapper(gym.Wrapper):
+    """
+    Overview:
+       Only death and pass sparse reward
+    Interface:
+        ``__init__``, ``step``
+    Properties:
+        - env (:obj:`gym.Env`): the environment to wrap.
+    """
+
+    def __init__(self, env: gym.Env):
+        super().__init__(env)
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+        dead = True if reward == -15 else False
+        reward = 0
+        if info['flag_get']:
+            reward = 15
+        if dead:
+            reward = -15
+        return obs, reward, done, info
+
+
+# 硬币奖励wrapper
+class CoinRewardWrapper(gym.Wrapper):
+    """
+    Overview:
+        add coin reward
+    Interface:
+        ``__init__``, ``step``
+    Properties:
+        - env (:obj:`gym.Env`): the environment to wrap.
+    """
+
+    def __init__(self, env: gym.Env):
+        super().__init__(env)
+        self.num_coins = 0
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+        reward += (info['coins'] - self.num_coins) * 10
+        self.num_coins = info['coins']
+        return obs, reward, done, info
+
+
+# CAM相关，不需要了解
+def dump_arr2video(arr, video_folder):
+    fourcc = cv2.VideoWriter_fourcc(*'MP4V')
+    fps = 6
+    size = (256, 240)
+    out = cv2.VideoWriter(video_folder + '/cam_pure.mp4', fourcc, fps, size)
+    out1 = cv2.VideoWriter(video_folder + '/obs_pure.mp4', fourcc, fps, size)
+    out2 = cv2.VideoWriter(video_folder + '/merged.mp4', fourcc, fps, size)
+    for frame, obs in arr:
+        frame = (255 * frame).astype('uint8').squeeze(0)
+        frame_c = cv2.resize(cv2.applyColorMap(frame, cv2.COLORMAP_JET), size)
+        out.write(frame_c)
+
+        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)
+        out1.write(obs)
+
+        merged_frame = cv2.addWeighted(obs, 0.6, frame_c, 0.4, 0)
+        out2.write(merged_frame)
+    # assert False
+
+
+def get_cam(img, model):
+    target_layers = [model.encoder.main[0]]
+    input_tensor = torch.from_numpy(img).unsqueeze(0)
+
+    # Construct the CAM object once, and then re-use it on many images:
+    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)
+    targets = None
+
+    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.
+    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)
+
+    # In this example grayscale_cam has only one image in the batch:
+    return grayscale_cam
+
+
+def capped_cubic_video_schedule(episode_id):
+    if episode_id < 1000:
+        return int(round(episode_id ** (1.0 / 3))) ** 3 == episode_id
+    else:
+        return episode_id % 1000 == 0
+
+
+class RecordCAM(gym.Wrapper):
+
+    def __init__(
+        self,
+        env,
+        cam_model,
+        video_folder: str,
+        episode_trigger: Callable[[int], bool] = None,
+        step_trigger: Callable[[int], bool] = None,
+        video_length: int = 0,
+        name_prefix: str = "rl-video",
+    ):
+        super(RecordCAM, self).__init__(env)
+        self._env = env
+        self.cam_model = cam_model
+
+        if episode_trigger is None and step_trigger is None:
+            episode_trigger = capped_cubic_video_schedule
+
+        trigger_count = sum([x is not None for x in [episode_trigger, step_trigger]])
+        assert trigger_count == 1, "Must specify exactly one trigger"
+
+        self.episode_trigger = episode_trigger
+        self.step_trigger = step_trigger
+        self.video_recorder = []
+
+        self.video_folder = os.path.abspath(video_folder)
+        # Create output folder if needed
+        if os.path.isdir(self.video_folder):
+            warnings.warn(
+                f"Overwriting existing videos at {self.video_folder} folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)"
+            )
+        os.makedirs(self.video_folder, exist_ok=True)
+
+        self.name_prefix = name_prefix
+        self.step_id = 0
+        self.video_length = video_length
+
+        self.recording = False
+        self.recorded_frames = 0
+        self.is_vector_env = getattr(env, "is_vector_env", False)
+        self.episode_id = 0
+
+    def reset(self, **kwargs):
+        observations = super(RecordCAM, self).reset(**kwargs)
+        if not self.recording:
+            self.start_video_recorder()
+        return observations
+
+    def start_video_recorder(self):
+        self.close_video_recorder()
+
+        video_name = f"{self.name_prefix}-step-{self.step_id}"
+        if self.episode_trigger:
+            video_name = f"{self.name_prefix}-episode-{self.episode_id}"
+
+        base_path = os.path.join(self.video_folder, video_name)
+        self.video_recorder = []
+
+        self.recorded_frames = 0
+        self.recording = True
+
+    def _video_enabled(self):
+        if self.step_trigger:
+            return self.step_trigger(self.step_id)
+        else:
+            return self.episode_trigger(self.episode_id)
+
+    def step(self, action):
+        time_step = super(RecordCAM, self).step(action)
+        observations, rewards, dones, infos = time_step
+
+        # increment steps and episodes
+        self.step_id += 1
+        if not self.is_vector_env:
+            if dones:
+                self.episode_id += 1
+        elif dones[0]:
+            self.episode_id += 1
+
+        if self.recording:
+            self.video_recorder.append(
+                (get_cam(observations, model=self.cam_model), copy.deepcopy(self.env.render(mode='rgb_array')))
+            )
+            self.recorded_frames += 1
+            if self.video_length > 0:
+                if self.recorded_frames > 10000:
+                    self.close_video_recorder()
+            else:
+                if not self.is_vector_env:
+                    if dones or infos['time'] < 250:
+                        self.close_video_recorder()
+                elif dones[0]:
+                    self.close_video_recorder()
+
+        elif self._video_enabled():
+            self.start_video_recorder()
+
+        return time_step
+
+    def close_video_recorder(self) -> None:
+        if self.recorded_frames > 0:
+            dump_arr2video(self.video_recorder, self.video_folder)
+        self.video_recorder = []
+        self.recording = False
+        self.recorded_frames = 0
+
+    def seed(self, seed: int) -> None:
         self._env.seed(seed)
\ No newline at end of file